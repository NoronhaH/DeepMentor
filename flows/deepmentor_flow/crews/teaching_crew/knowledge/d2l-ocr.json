{
  "summary": {
    "3": {
      "text": "Contents\n\nPreface  page xxv\n\nInstallation xxxiv\n\nNotation xxxvii\n\n1 Introduction 1\n\n1.1 A Motivating Example 2\n\n1.2 Key Components 4\n\n1.3 Kinds of Machine Learning Problems 7\n\n1.4 Roots 20\n\n1.5 The Road to Deep Learning 22\n\n1.6 Success Stories 25\n\n1.7 The Essence of Deep Learning 27\n\n1.8 Summary 29\n\n1.9 Exercises 29\n\n2 Preliminaries 30\n\n2.1 Data Manipulation 30\n\n2.1.1 Getting Started 30\n\n2.1.2 Indexing and Slicing 33\n\n2.1.3 Operations 34\n\n2.1.4 Broadcasting 35\n\n2.1.5 Saving Memory 36\n\n2.1.6 Conversion to Other Python Objects 37\n\n2.1.7 Summary 37\n\n2.1.8 Exercises 38\n\n2.2 Data Preprocessing 38\n\n2.2.1 Reading the Dataset 38\n\n2.2.2 Data Preparation 39\n\n2.2.3 Conversion to the Tensor Format 40\n\n2.2.4 Discussion 40\n\n2.2.5 Exercises 40\n\n2.3 Linear Algebra 41\n\n2.3.1 Scalars 41"
    },
    "4": {
      "text": "2.3.2 Vectors 42\n2.3.3 Matrices 43\n2.3.4 Tensors 44\n2.3.5 Basic Properties of Tensor Arithmetic 45\n2.3.6 Reduction 46\n2.3.7 Non-Reduction Sum 47\n2.3.8 Dot Products 48\n2.3.9 Matrix–Vector Products 48\n2.3.10 Matrix–Matrix Multiplication 49\n2.3.11 Norms 50\n2.3.12 Discussion 52\n2.3.13 Exercises 53\n2.4 Calculus 54\n2.4.1 Derivatives and Differentiation 54\n2.4.2 Visualization Utilities 56\n2.4.3 Partial Derivatives and Gradients 58\n2.4.4 Chain Rule 58\n2.4.5 Discussion 59\n2.4.6 Exercises 59\n2.5 Automatic Differentiation 60\n2.5.1 A Simple Function 60\n2.5.2 Backward for Non-Scalar Variables 61\n2.5.3 Detaching Computation 62\n2.5.4 Gradients and Python Control Flow 63\n2.5.5 Discussion 64\n2.5.6 Exercises 64\n2.6 Probability and Statistics 65\n2.6.1 A Simple Example: Tossing Coins 66\n2.6.2 A More Formal Treatment 68\n2.6.3 Random Variables 69\n2.6.4 Multiple Random Variables 70\n2.6.5 An Example 73\n2.6.6 Expectations 74\n2.6.7 Discussion 76\n2.6.8 Exercises 77\n2.7 Documentation 78\n2.7.1 Functions and Classes in a Module 78\n2.7.2 Specific Functions and Classes 79\n3 Linear Neural Networks for Regression 82\n3.1 Linear Regression 82\n3.1.1 Basics 83\n3.1.2 Vectorization for Speed 88\n3.1.3 The Normal Distribution and Squared Loss 88\n3.1.4 Linear Regression as a Neural Network 90"
    },
    "5": {
      "text": "3.1.5 Summary 91\n3.1.6 Exercises 92\n3.2 Object-Oriented Design for Implementation 93\n3.2.1 Utilities 94\n3.2.2 Models 96\n3.2.3 Data 97\n3.2.4 Training 97\n3.2.5 Summary 98\n3.2.6 Exercises 98\n3.3 Synthetic Regression Data 99\n3.3.1 Generating the Dataset 99\n3.3.2 Reading the Dataset 100\n3.3.3 Concise Implementation of the Data Loader 101\n3.3.4 Summary 102\n3.3.5 Exercises 102\n3.4 Linear Regression Implementation from Scratch 103\n3.4.1 Defining the Model 103\n3.4.2 Defining the Loss Function 104\n3.4.3 Defining the Optimization Algorithm 104\n3.4.4 Training 105\n3.4.5 Summary 107\n3.4.6 Exercises 107\n3.5 Concise Implementation of Linear Regression 108\n3.5.1 Defining the Model 109\n3.5.2 Defining the Loss Function 109\n3.5.3 Defining the Optimization Algorithm 110\n3.5.4 Training 110\n3.5.5 Summary 111\n3.5.6 Exercises 111\n3.6 Generalization 112\n3.6.1 Training Error and Generalization Error 113\n3.6.2 Underfitting or Overfitting? 115\n3.6.3 Model Selection 116\n3.6.4 Summary 117\n3.6.5 Exercises 117\n3.7 Weight Decay 118\n3.7.1 Norms and Weight Decay 119\n3.7.2 High-Dimensional Linear Regression 120\n3.7.3 Implementation from Scratch 121\n3.7.4 Concise Implementation 122\n3.7.5 Summary 124\n3.7.6 Exercises 124\n4 Linear Neural Networks for Classification 125\n4.1 Softmax Regression 125"
    },
    "6": {
      "text": "4.1.1 Classification 126\n4.1.2 Loss Function 129\n4.1.3 Information Theory Basics 130\n4.1.4 Summary and Discussion 131\n4.1.5 Exercises 132\n4.2 The Image Classification Dataset 134\n4.2.1 Loading the Dataset 134\n4.2.2 Reading a Minibatch 135\n4.2.3 Visualization 136\n4.2.4 Summary 137\n4.2.5 Exercises 137\n4.3 The Base Classification Model 138\n4.3.1 The Classifier Class 138\n4.3.2 Accuracy 138\n4.3.3 Summary 139\n4.3.4 Exercises 139\n4.4 Softmax Regression Implementation from Scratch 140\n4.4.1 The Softmax 140\n4.4.2 The Model 141\n4.4.3 The Cross-Entropy Loss 141\n4.4.4 Training 142\n4.4.5 Prediction 143\n4.4.6 Summary 143\n4.4.7 Exercises 144\n4.5 Concise Implementation of Softmax Regression 144\n4.5.1 Defining the Model 145\n4.5.2 Softmax Revisited 145\n4.5.3 Training 146\n4.5.4 Summary 146\n4.5.5 Exercises 147\n4.6 Generalization in Classification 147\n4.6.1 The Test Set 148\n4.6.2 Test Set Reuse 150\n4.6.3 Statistical Learning Theory 151\n4.6.4 Summary 153\n4.6.5 Exercises 154\n4.7 Environment and Distribution Shift 154\n4.7.1 Types of Distribution Shift 155\n4.7.2 Examples of Distribution Shift 157\n4.7.3 Correction of Distribution Shift 159\n4.7.4 A Taxonomy of Learning Problems 163\n4.7.5 Fairness, Accountability, and Transparency in Machine Learning 164\n4.7.6 Summary 165\n4.7.7 Exercises 166"
    },
    "7": {
      "text": "5 Multilayer Perceptrons 167\n5.1 Multilayer Perceptrons 167\n5.1.1 Hidden Layers 167\n5.1.2 Activation Functions 171\n5.1.3 Summary and Discussion 174\n5.1.4 Exercises 175\n5.2 Implementation of Multilayer Perceptrons 176\n5.2.1 Implementation from Scratch 176\n5.2.2 Concise Implementation 177\n5.2.3 Summary 178\n5.2.4 Exercises 179\n5.3 Forward Propagation, Backward Propagation, and Computational Graphs 180\n5.3.1 Forward Propagation 180\n5.3.2 Computational Graph of Forward Propagation 181\n5.3.3 Backpropagation 181\n5.3.4 Training Neural Networks 183\n5.3.5 Summary 183\n5.3.6 Exercises 183\n5.4 Numerical Stability and Initialization 184\n5.4.1 Vanishing and Exploding Gradients 184\n5.4.2 Parameter Initialization 187\n5.4.3 Summary 188\n5.4.4 Exercises 189\n5.5 Generalization in Deep Learning 189\n5.5.1 Revisiting Overfitting and Regularization 190\n5.5.2 Inspiration from Nonparametrics 191\n5.5.3 Early Stopping 192\n5.5.4 Classical Regularization Methods for Deep Networks 193\n5.5.5 Summary 193\n5.5.6 Exercises 194\n5.6 Dropout 194\n5.6.1 Dropout in Practice 195\n5.6.2 Implementation from Scratch 196\n5.6.3 Concise Implementation 197\n5.6.4 Summary 198\n5.6.5 Exercises 198\n5.7 Predicting House Prices on Kaggle 199\n5.7.1 Downloading Data 199\n5.7.2 Kaggle 200\n5.7.3 Accessing and Reading the Dataset 201\n5.7.4 Data Preprocessing 201\n5.7.5 Error Measure 203\n5.7.6 K-Fold Cross-Validation 204\n5.7.7 Model Selection 204\n5.7.8 Submitting Predictions on Kaggle 205"
    },
    "8": {
      "text": "5.7.9 Summary and Discussion 206\n5.7.10 Exercises 206\n\n6 Builders’ Guide 207\n6.1 Layers and Modules 207\n6.1.1 A Custom Module 209\n6.1.2 The Sequential Module 211\n6.1.3 Executing Code in the Forward Propagation Method 211\n6.1.4 Summary 213\n6.1.5 Exercises 213\n6.2 Parameter Management 213\n6.2.1 Parameter Access 214\n6.2.2 Tied Parameters 215\n6.2.3 Summary 216\n6.2.4 Exercises 216\n6.3 Parameter Initialization 216\n6.3.1 Built-in Initialization 217\n6.3.2 Summary 219\n6.3.3 Exercises 219\n6.4 Lazy Initialization 219\n6.4.1 Summary 220\n6.4.2 Exercises 221\n6.5 Custom Layers 221\n6.5.1 Layers without Parameters 221\n6.5.2 Layers with Parameters 222\n6.5.3 Summary 223\n6.5.4 Exercises 223\n6.6 File I/O 223\n6.6.1 Loading and Saving Tensors 224\n6.6.2 Loading and Saving Model Parameters 225\n6.6.3 Summary 226\n6.6.4 Exercises 226\n6.7 GPUs 226\n6.7.1 Computing Devices 227\n6.7.2 Tensors and GPUs 228\n6.7.3 Neural Networks and GPUs 230\n6.7.4 Summary 231\n6.7.5 Exercises 231\n\n7 Convolutional Neural Networks 233\n7.1 From Fully Connected Layers to Convolutions 234\n7.1.1 Invariance 234\n7.1.2 Constraining the MLP 235\n7.1.3 Convolutions 237\n7.1.4 Channels 238"
    },
    "9": {
      "text": "7.1.5 Summary and Discussion 239\n7.1.6 Exercises 239\n7.2 Convolutions for Images 240\n7.2.1 The Cross-Correlation Operation 240\n7.2.2 Convolutional Layers 242\n7.2.3 Object Edge Detection in Images 242\n7.2.4 Learning a Kernel 244\n7.2.5 Cross-Correlation and Convolution 245\n7.2.6 Feature Map and Receptive Field 245\n7.2.7 Summary 246\n7.2.8 Exercises 247\n7.3 Padding and Stride 247\n7.3.1 Padding 248\n7.3.2 Stride 250\n7.3.3 Summary and Discussion 251\n7.3.4 Exercises 251\n7.4 Multiple Input and Multiple Output Channels 252\n7.4.1 Multiple Input Channels 252\n7.4.2 Multiple Output Channels 253\n7.4.3 1 × 1 Convolutional Layer 255\n7.4.4 Discussion 256\n7.4.5 Exercises 256\n7.5 Pooling 257\n7.5.1 Maximum Pooling and Average Pooling 258\n7.5.2 Padding and Stride 260\n7.5.3 Multiple Channels 261\n7.5.4 Summary 261\n7.5.5 Exercises 262\n7.6 Convolutional Neural Networks (LeNet) 262\n7.6.1 LeNet 263\n7.6.2 Training 265\n7.6.3 Summary 266\n7.6.4 Exercises 266\n8 Modern Convolutional Neural Networks 268\n8.1 Deep Convolutional Neural Networks (AlexNet) 269\n8.1.1 Representation Learning 270\n8.1.2 AlexNet 273\n8.1.3 Training 276\n8.1.4 Discussion 276\n8.1.5 Exercises 277\n8.2 Networks Using Blocks (VGG) 278\n8.2.1 VGG Blocks 279\n8.2.2 VGG Network 279\n8.2.3 Training 281"
    },
    "10": {
      "text": "8.2.4 Summary 282\n8.2.5 Exercises 282\n8.3 Network in Network (NiN) 283\n8.3.1 NiN Blocks 283\n8.3.2 NiN Model 284\n8.3.3 Training 285\n8.3.4 Summary 286\n8.3.5 Exercises 286\n8.4 Multi-Branch Networks (GoogLeNet) 287\n8.4.1 Inception Blocks 287\n8.4.2 GoogLeNet Model 288\n8.4.3 Training 291\n8.4.4 Discussion 291\n8.4.5 Exercises 292\n8.5 Batch Normalization 292\n8.5.1 Training Deep Networks 293\n8.5.2 Batch Normalization Layers 295\n8.5.3 Implementation from Scratch 297\n8.5.4 LeNet with Batch Normalization 298\n8.5.5 Concise Implementation 299\n8.5.6 Discussion 300\n8.5.7 Exercises 301\n8.6 Residual Networks (ResNet) and ResNeXt 302\n8.6.1 Function Classes 302\n8.6.2 Residual Blocks 304\n8.6.3 ResNet Model 306\n8.6.4 Training 308\n8.6.5 ResNeXt 308\n8.6.6 Summary and Discussion 310\n8.6.7 Exercises 311\n8.7 Densely Connected Networks (DenseNet) 312\n8.7.1 From ResNet to DenseNet 312\n8.7.2 Dense Blocks 313\n8.7.3 Transition Layers 314\n8.7.4 DenseNet Model 315\n8.7.5 Training 315\n8.7.6 Summary and Discussion 316\n8.7.7 Exercises 316\n8.8 Designing Convolution Network Architectures 317\n8.8.1 The AnyNet Design Space 318\n8.8.2 Distributions and Parameters of Design Spaces 320\n8.8.3 RegNet 322\n8.8.4 Training 323\n8.8.5 Discussion 323\n8.8.6 Exercises 324"
    },
    "11": {
      "text": "9 Recurrent Neural Networks 325\n\n9.1 Working with Sequences 327\n\n9.1.1 Autoregressive Models 328\n\n9.1.2 Sequence Models 330\n\n9.1.3 Training 331\n\n9.1.4 Prediction 333\n\n9.1.5 Summary 335\n\n9.1.6 Exercises 335\n\n9.2 Converting Raw Text into Sequence Data 336\n\n9.2.1 Reading the Dataset 336\n\n9.2.2 Tokenization 337\n\n9.2.3 Vocabulary 337\n\n9.2.4 Putting It All Together 338\n\n9.2.5 Exploratory Language Statistics 339\n\n9.2.6 Summary 341\n\n9.2.7 Exercises 342\n\n9.3 Language Models 342\n\n9.3.1 Learning Language Models 343\n\n9.3.2 Perplexity 345\n\n9.3.3 Partitioning Sequences 346\n\n9.3.4 Summary and Discussion 347\n\n9.3.5 Exercises 348\n\n9.4 Recurrent Neural Networks 348\n\n9.4.1 Neural Networks without Hidden States 349\n\n9.4.2 Recurrent Neural Networks with Hidden States 349\n\n9.4.3 RNN-Based Character-Level Language Models 351\n\n9.4.4 Summary 352\n\n9.4.5 Exercises 352\n\n9.5 Recurrent Neural Network Implementation from Scratch 352\n\n9.5.1 RNN Model 353\n\n9.5.2 RNN-Based Language Model 354\n\n9.5.3 Gradient Clipping 356\n\n9.5.4 Training 357\n\n9.5.5 Decoding 358\n\n9.5.6 Summary 359\n\n9.5.7 Exercises 359\n\n9.6 Concise Implementation of Recurrent Neural Networks 360\n\n9.6.1 Defining the Model 360\n\n9.6.2 Training and Predicting 361\n\n9.6.3 Summary 362\n\n9.6.4 Exercises 362\n\n9.7 Backpropagation Through Time 362\n\n9.7.1 Analysis of Gradients in RNNs 362\n\n9.7.2 Backpropagation Through Time in Detail 365\n\n9.7.3 Summary 368"
    },
    "12": {
      "text": "9.7.4 Exercises 368\n\n10 Modern Recurrent Neural Networks 369\n10.1 Long Short-Term Memory (LSTM) 370\n10.1.1 Gated Memory Cell 370\n10.1.2 Implementation from Scratch 373\n10.1.3 Concise Implementation 375\n10.1.4 Summary 376\n10.1.5 Exercises 376\n10.2 Gated Recurrent Units (GRU) 376\n10.2.1 Reset Gate and Update Gate 377\n10.2.2 Candidate Hidden State 378\n10.2.3 Hidden State 378\n10.2.4 Implementation from Scratch 379\n10.2.5 Concise Implementation 380\n10.2.6 Summary 381\n10.2.7 Exercises 381\n10.3 Deep Recurrent Neural Networks 382\n10.3.1 Implementation from Scratch 383\n10.3.2 Concise Implementation 384\n10.3.3 Summary 385\n10.3.4 Exercises 385\n10.4 Bidirectional Recurrent Neural Networks 385\n10.4.1 Implementation from Scratch 387\n10.4.2 Concise Implementation 387\n10.4.3 Summary 388\n10.4.4 Exercises 388\n10.5 Machine Translation and the Dataset 388\n10.5.1 Downloading and Preprocessing the Dataset 389\n10.5.2 Tokenization 390\n10.5.3 Loading Sequences of Fixed Length 391\n10.5.4 Reading the Dataset 392\n10.5.5 Summary 393\n10.5.6 Exercises 394\n10.6 The Encoder–Decoder Architecture 394\n10.6.1 Encoder 394\n10.6.2 Decoder 395\n10.6.3 Putting the Encoder and Decoder Together 395\n10.6.4 Summary 396\n10.6.5 Exercises 396\n10.7 Sequence-to-Sequence Learning for Machine Translation 396\n10.7.1 Teacher Forcing 397\n10.7.2 Encoder 397\n10.7.3 Decoder 399\n10.7.4 Encoder–Decoder for Sequence-to-Sequence Learning 400"
    },
    "13": {
      "text": "10.7.5 Loss Function with Masking 401\n10.7.6 Training 401\n10.7.7 Prediction 402\n10.7.8 Evaluation of Predicted Sequences 403\n10.7.9 Summary 404\n10.7.10 Exercises 404\n10.8 Beam Search 405\n10.8.1 Greedy Search 405\n10.8.2 Exhaustive Search 407\n10.8.3 Beam Search 407\n10.8.4 Summary 408\n10.8.5 Exercises 408\n11 Attention Mechanisms and Transformers 409\n11.1 Queries, Keys, and Values 411\n11.1.1 Visualization 413\n11.1.2 Summary 414\n11.1.3 Exercises 414\n11.2 Attention Pooling by Similarity 415\n11.2.1 Kernels and Data 415\n11.2.2 Attention Pooling via Nadaraya–Watson Regression 417\n11.2.3 Adapting Attention Pooling 418\n11.2.4 Summary 419\n11.2.5 Exercises 420\n11.3 Attention Scoring Functions 420\n11.3.1 Dot Product Attention 421\n11.3.2 Convenience Functions 421\n11.3.3 Scaled Dot Product Attention 423\n11.3.4 Additive Attention 424\n11.3.5 Summary 426\n11.3.6 Exercises 426\n11.4 The Bahdanau Attention Mechanism 427\n11.4.1 Model 428\n11.4.2 Defining the Decoder with Attention 428\n11.4.3 Training 430\n11.4.4 Summary 431\n11.4.5 Exercises 432\n11.5 Multi-Head Attention 432\n11.5.1 Model 433\n11.5.2 Implementation 433\n11.5.3 Summary 435\n11.5.4 Exercises 435\n11.6 Self-Attention and Positional Encoding 435\n11.6.1 Self-Attention 436\n11.6.2 Comparing CNNs, RNNs, and Self-Attention 436"
    },
    "14": {
      "text": "11.6.3 Positional Encoding 437\n11.6.4 Summary 440\n11.6.5 Exercises 440\n11.7 The Transformer Architecture 440\n11.7.1 Model 441\n11.7.2 Positionwise Feed-Forward Networks 442\n11.7.3 Residual Connection and Layer Normalization 443\n11.7.4 Encoder 444\n11.7.5 Decoder 445\n11.7.6 Training 447\n11.7.7 Summary 451\n11.7.8 Exercises 451\n11.8 Transformers for Vision 451\n11.8.1 Model 452\n11.8.2 Patch Embedding 453\n11.8.3 Vision Transformer Encoder 453\n11.8.4 Putting It All Together 454\n11.8.5 Training 455\n11.8.6 Summary and Discussion 455\n11.8.7 Exercises 456\n11.9 Large-Scale Pretraining with Transformers 456\n11.9.1 Encoder-Only 457\n11.9.2 Encoder–Decoder 459\n11.9.3 Decoder-Only 461\n11.9.4 Scalability 463\n11.9.5 Large Language Models 465\n11.9.6 Summary and Discussion 466\n11.9.7 Exercises 467\n12 Optimization Algorithms 468\n12.1 Optimization and Deep Learning 468\n12.1.1 Goal of Optimization 469\n12.1.2 Optimization Challenges in Deep Learning 469\n12.1.3 Summary 473\n12.1.4 Exercises 473\n12.2 Convexity 474\n12.2.1 Definitions 474\n12.2.2 Properties 476\n12.2.3 Constraints 479\n12.2.4 Summary 481\n12.2.5 Exercises 482\n12.3 Gradient Descent 482\n12.3.1 One-Dimensional Gradient Descent 482\n12.3.2 Multivariate Gradient Descent 486\n12.3.3 Adaptive Methods 488"
    },
    "15": {
      "text": "12.3.4 Summary 492\n12.3.5 Exercises 492\n12.4 Stochastic Gradient Descent 493\n12.4.1 Stochastic Gradient Updates 493\n12.4.2 Dynamic Learning Rate 495\n12.4.3 Convergence Analysis for Convex Objectives 496\n12.4.4 Stochastic Gradients and Finite Samples 498\n12.4.5 Summary 499\n12.4.6 Exercises 499\n12.5 Minibatch Stochastic Gradient Descent 500\n12.5.1 Vectorization and Caches 500\n12.5.2 Minibatches 503\n12.5.3 Reading the Dataset 504\n12.5.4 Implementation from Scratch 504\n12.5.5 Concise Implementation 507\n12.5.6 Summary 509\n12.5.7 Exercises 509\n12.6 Momentum 510\n12.6.1 Basics 510\n12.6.2 Practical Experiments 514\n12.6.3 Theoretical Analysis 516\n12.6.4 Summary 518\n12.6.5 Exercises 519\n12.7 Adagrad 519\n12.7.1 Sparse Features and Learning Rates 519\n12.7.2 Preconditioning 520\n12.7.3 The Algorithm 521\n12.7.4 Implementation from Scratch 523\n12.7.5 Concise Implementation 524\n12.7.6 Summary 524\n12.7.7 Exercises 525\n12.8 RMSProp 525\n12.8.1 The Algorithm 526\n12.8.2 Implementation from Scratch 526\n12.8.3 Concise Implementation 528\n12.8.4 Summary 528\n12.8.5 Exercises 529\n12.9 Adadelta 529\n12.9.1 The Algorithm 529\n12.9.2 Implementation 530\n12.9.3 Summary 531\n12.9.4 Exercises 532\n12.10 Adam 532\n12.10.1 The Algorithm 532\n12.10.2 Implementation 533"
    },
    "16": {
      "text": "12.10.3 Yogi 534\n12.10.4 Summary 535\n12.10.5 Exercises 536\n12.11 Learning Rate Scheduling 536\n12.11.1 Toy Problem 537\n12.11.2 Schedulers 539\n12.11.3 Policies 540\n12.11.4 Summary 545\n12.11.5 Exercises 545\n13 Computational Performance 547\n13.1 Compilers and Interpreters 547\n13.1.1 Symbolic Programming 548\n13.1.2 Hybrid Programming 549\n13.1.3 Hybridizing the Sequential Class 550\n13.1.4 Summary 552\n13.1.5 Exercises 552\n13.2 Asynchronous Computation 552\n13.2.1 Asynchrony via Backend 553\n13.2.2 Barriers and Blockers 554\n13.2.3 Improving Computation 555\n13.2.4 Summary 555\n13.2.5 Exercises 555\n13.3 Automatic Parallelism 555\n13.3.1 Parallel Computation on GPUs 556\n13.3.2 Parallel Computation and Communication 557\n13.3.3 Summary 558\n13.3.4 Exercises 559\n13.4 Hardware 559\n13.4.1 Computers 560\n13.4.2 Memory 561\n13.4.3 Storage 562\n13.4.4 CPUs 563\n13.4.5 GPUs and other Accelerators 566\n13.4.6 Networks and Buses 569\n13.4.7 More Latency Numbers 570\n13.4.8 Summary 571\n13.4.9 Exercises 571\n13.5 Training on Multiple GPUs 572\n13.5.1 Splitting the Problem 573\n13.5.2 Data Parallelism 574\n13.5.3 A Toy Network 575\n13.5.4 Data Synchronization 576\n13.5.5 Distributing Data 577\n13.5.6 Training 578"
    },
    "17": {
      "text": "13.5.7 Summary 580\n13.5.8 Exercises 580\n13.6 Concise Implementation for Multiple GPUs 581\n13.6.1 A Toy Network 581\n13.6.2 Network Initialization 582\n13.6.3 Training 582\n13.6.4 Summary 583\n13.6.5 Exercises 584\n13.7 Parameter Servers 584\n13.7.1 Data-Parallel Training 584\n13.7.2 Ring Synchronization 586\n13.7.3 Multi-Machine Training 588\n13.7.4 Key–Value Stores 589\n13.7.5 Summary 591\n13.7.6 Exercises 591\n\n14 Computer Vision 592\n14.1 Image Augmentation 592\n14.1.1 Common Image Augmentation Methods 593\n14.1.2 Training with Image Augmentation 596\n14.1.3 Summary 599\n14.1.4 Exercises 599\n14.2 Fine-Tuning 600\n14.2.1 Steps 600\n14.2.2 Hot Dog Recognition 601\n14.2.3 Summary 605\n14.2.4 Exercises 606\n14.3 Object Detection and Bounding Boxes 606\n14.3.1 Bounding Boxes 607\n14.3.2 Summary 609\n14.3.3 Exercises 609\n14.4 Anchor Boxes 609\n14.4.1 Generating Multiple Anchor Boxes 610\n14.4.2 Intersection over Union (IoU) 612\n14.4.3 Labeling Anchor Boxes in Training Data 613\n14.4.4 Predicting Bounding Boxes with Non-Maximum Suppression 619\n14.4.5 Summary 622\n14.4.6 Exercises 623\n14.5 Multiscale Object Detection 623\n14.5.1 Multiscale Anchor Boxes 623\n14.5.2 Multiscale Detection 625\n14.5.3 Summary 626\n14.5.4 Exercises 626\n14.6 The Object Detection Dataset 627\n14.6.1 Downloading the Dataset 627"
    },
    "18": {
      "text": "14.6.2 Reading the Dataset 627\n14.6.3 Demonstration 629\n14.6.4 Summary 629\n14.6.5 Exercises 630\n14.7 Single Shot Multibox Detection 630\n14.7.1 Model 630\n14.7.2 Training 636\n14.7.3 Prediction 638\n14.7.4 Summary 639\n14.7.5 Exercises 640\n14.8 Region-based CNNs (R-CNNs) 642\n14.8.1 R-CNNs 642\n14.8.2 Fast R-CNN 643\n14.8.3 Faster R-CNN 645\n14.8.4 Mask R-CNN 646\n14.8.5 Summary 647\n14.8.6 Exercises 647\n14.9 Semantic Segmentation and the Dataset 648\n14.9.1 Image Segmentation and Instance Segmentation 648\n14.9.2 The Pascal VOC2012 Semantic Segmentation Dataset 648\n14.9.3 Summary 654\n14.9.4 Exercises 654\n14.10 Transposed Convolution 654\n14.10.1 Basic Operation 654\n14.10.2 Padding, Strides, and Multiple Channels 656\n14.10.3 Connection to Matrix Transposition 657\n14.10.4 Summary 659\n14.10.5 Exercises 659\n14.11 Fully Convolutional Networks 659\n14.11.1 The Model 660\n14.11.2 Initializing Transposed Convolutional Layers 662\n14.11.3 Reading the Dataset 663\n14.11.4 Training 664\n14.11.5 Prediction 664\n14.11.6 Summary 666\n14.11.7 Exercises 666\n14.12 Neural Style Transfer 666\n14.12.1 Method 666\n14.12.2 Reading the Content and Style Images 668\n14.12.3 Preprocessing and Postprocessing 668\n14.12.4 Extracting Features 669\n14.12.5 Defining the Loss Function 670\n14.12.6 Initializing the Synthesized Image 672\n14.12.7 Training 673\n14.12.8 Summary 674"
    },
    "19": {
      "text": "14.12.9 Exercises 674\n14.13 Image Classification (CIFAR-10) on Kaggle 674\n14.13.1 Obtaining and Organizing the Dataset 675\n14.13.2 Image Augmentation 678\n14.13.3 Reading the Dataset 678\n14.13.4 Defining the Model 679\n14.13.5 Defining the Training Function 679\n14.13.6 Training and Validating the Model 680\n14.13.7 Classifying the Testing Set and Submitting Results on Kaggle 680\n14.13.8 Summary 681\n14.13.9 Exercises 682\n14.14 Dog Breed Identification (ImageNet Dogs) on Kaggle 682\n14.14.1 Obtaining and Organizing the Dataset 682\n14.14.2 Image Augmentation 684\n14.14.3 Reading the Dataset 685\n14.14.4 Fine-Tuning a Pretrained Model 685\n14.14.5 Defining the Training Function 686\n14.14.6 Training and Validating the Model 687\n14.14.7 Classifying the Testing Set and Submitting Results on Kaggle 688\n14.14.8 Summary 688\n14.14.9 Exercises 689\n15 Natural Language Processing: Pretraining 690\n15.1 Word Embedding (word2vec) 691\n15.1.1 One-Hot Vectors Are a Bad Choice 691\n15.1.2 Self-Supervised word2vec 691\n15.1.3 The Skip-Gram Model 692\n15.1.4 The Continuous Bag of Words (CBOW) Model 694\n15.1.5 Summary 695\n15.1.6 Exercises 695\n15.2 Approximate Training 696\n15.2.1 Negative Sampling 696\n15.2.2 Hierarchical Softmax 698\n15.2.3 Summary 699\n15.2.4 Exercises 699\n15.3 The Dataset for Pretraining Word Embeddings 699\n15.3.1 Reading the Dataset 699\n15.3.2 Subsampling 700\n15.3.3 Extracting Center Words and Context Words 702\n15.3.4 Negative Sampling 703\n15.3.5 Loading Training Examples in Minibatches 704\n15.3.6 Putting It All Together 705\n15.3.7 Summary 706\n15.3.8 Exercises 706\n15.4 Pretraining word2vec 707"
    },
    "20": {
      "text": "15.4.1 The Skip-Gram Model 707\n15.4.2 Training 708\n15.4.3 Applying Word Embeddings 711\n15.4.4 Summary 711\n15.4.5 Exercises 711\n15.5 Word Embedding with Global Vectors (GloVe) 711\n15.5.1 Skip-Gram with Global Corpus Statistics 712\n15.5.2 The GloVe Model 713\n15.5.3 Interpreting GloVe from the Ratio of Co-occurrence Probabilities 713\n15.5.4 Summary 715\n15.5.5 Exercises 715\n15.6 Subword Embedding 715\n15.6.1 The fastText Model 715\n15.6.2 Byte Pair Encoding 716\n15.6.3 Summary 719\n15.6.4 Exercises 719\n15.7 Word Similarity and Analogy 720\n15.7.1 Loading Pretrained Word Vectors 720\n15.7.2 Applying Pretrained Word Vectors 722\n15.7.3 Summary 724\n15.7.4 Exercises 724\n15.8 Bidirectional Encoder Representations from Transformers (BERT) 724\n15.8.1 From Context-Independent to Context-Sensitive 724\n15.8.2 From Task-Specific to Task-Agnostic 725\n15.8.3 BERT: Combining the Best of Both Worlds 725\n15.8.4 Input Representation 726\n15.8.5 Pretraining Tasks 728\n15.8.6 Putting It All Together 731\n15.8.7 Summary 732\n15.8.8 Exercises 733\n15.9 The Dataset for Pretraining BERT 733\n15.9.1 Defining Helper Functions for Pretraining Tasks 734\n15.9.2 Transforming Text into the Pretraining Dataset 736\n15.9.3 Summary 738\n15.9.4 Exercises 739\n15.10 Pretraining BERT 739\n15.10.1 Pretraining BERT 739\n15.10.2 Representing Text with BERT 741\n15.10.3 Summary 742\n15.10.4 Exercises 743\n16 Natural Language Processing: Applications 744\n16.1 Sentiment Analysis and the Dataset 745\n16.1.1 Reading the Dataset 745"
    },
    "21": {
      "text": "16.1.2 Preprocessing the Dataset 746\n16.1.3 Creating Data Iterators 747\n16.1.4 Putting It All Together 747\n16.1.5 Summary 748\n16.1.6 Exercises 748\n16.2 Sentiment Analysis: Using Recurrent Neural Networks 748\n16.2.1 Representing Single Text with RNNs 749\n16.2.2 Loading Pretrained Word Vectors 750\n16.2.3 Training and Evaluating the Model 751\n16.2.4 Summary 751\n16.2.5 Exercises 752\n16.3 Sentiment Analysis: Using Convolutional Neural Networks 752\n16.3.1 One-Dimensional Convolutions 753\n16.3.2 Max-Over-Time Pooling 754\n16.3.3 The textCNN Model 755\n16.3.4 Summary 758\n16.3.5 Exercises 758\n16.4 Natural Language Inference and the Dataset 759\n16.4.1 Natural Language Inference 759\n16.4.2 The Stanford Natural Language Inference (SNLI) Dataset 760\n16.4.3 Summary 763\n16.4.4 Exercises 763\n16.5 Natural Language Inference: Using Attention 763\n16.5.1 The Model 764\n16.5.2 Training and Evaluating the Model 768\n16.5.3 Summary 770\n16.5.4 Exercises 770\n16.6 Fine-Tuning BERT for Sequence-Level and Token-Level Applications 771\n16.6.1 Single Text Classification 771\n16.6.2 Text Pair Classification or Regression 772\n16.6.3 Text Tagging 773\n16.6.4 Question Answering 773\n16.6.5 Summary 774\n16.6.6 Exercises 774\n16.7 Natural Language Inference: Fine-Tuning BERT 775\n16.7.1 Loading Pretrained BERT 775\n16.7.2 The Dataset for Fine-Tuning BERT 776\n16.7.3 Fine-Tuning BERT 778\n16.7.4 Summary 779\n16.7.5 Exercises 779\n17 Reinforcement Learning 781\n17.1 Markov Decision Process (MDP) 782\n17.1.1 Definition of an MDP 782\n17.1.2 Return and Discount Factor 783"
    },
    "22": {
      "text": "17.1.3 Discussion of the Markov Assumption 784\n17.1.4 Summary 785\n17.1.5 Exercises 785\n17.2 Value Iteration 785\n17.2.1 Stochastic Policy 785\n17.2.2 Value Function 786\n17.2.3 Action-Value Function 786\n17.2.4 Optimal Stochastic Policy 787\n17.2.5 Principle of Dynamic Programming 787\n17.2.6 Value Iteration 788\n17.2.7 Policy Evaluation 788\n17.2.8 Implementation of Value Iteration 789\n17.2.9 Summary 790\n17.2.10 Exercises 791\n17.3 Q-Learning 791\n17.3.1 The Q-Learning Algorithm 791\n17.3.2 An Optimization Problem Underlying Q-Learning 791\n17.3.3 Exploration in Q-Learning 793\n17.3.4 The “Self-correcting” Property of Q-Learning 793\n17.3.5 Implementation of Q-Learning 794\n17.3.6 Summary 795\n17.3.7 Exercises 796\n18 Gaussian Processes 797\n18.1 Introduction to Gaussian Processes 798\n18.1.1 Summary 807\n18.1.2 Exercises 808\n18.2 Gaussian Process Priors 809\n18.2.1 Definition 809\n18.2.2 A Simple Gaussian Process 810\n18.2.3 From Weight Space to Function Space 811\n18.2.4 The Radial Basis Function (RBF) Kernel 811\n18.2.5 The Neural Network Kernel 813\n18.2.6 Summary 814\n18.2.7 Exercises 814\n18.3 Gaussian Process Inference 815\n18.3.1 Posterior Inference for Regression 815\n18.3.2 Equations for Making Predictions and Learning Kernel Hyperparameters in GP Regression 817\n18.3.3 Interpreting Equations for Learning and Predictions 817\n18.3.4 Worked Example from Scratch 818\n18.3.5 Making Life Easy with GPyTorch 822\n18.3.6 Summary 825\n18.3.7 Exercises 826"
    },
    "23": {
      "text": "xxiii"
    },
    "24": {
      "text": "20.2.6 Exercises 892\n\n21 Recommender Systems 893\n\n21.1 Overview of Recommender Systems 893\n\n21.1.1 Collaborative Filtering 894\n\n21.1.2 Explicit Feedback and Implicit Feedback 895\n\n21.1.3 Recommendation Tasks 895\n\n21.1.4 Summary 895\n\n21.1.5 Exercises 895\n\nAppendix A Mathematics for Deep Learning 897\n\nAppendix B Tools for Deep Learning 1035\n\nReferences 1089"
    }
  },
  "chapter-3": {
    "122": {
      "text": "Before we worry about making our neural networks deep, it will be helpful to implement some shallow ones, for which the inputs connect directly to the outputs. This will prove important for a few reasons. First, rather than getting distracted by complicated architectures, we can focus on the basics of neural network training, including parametrizing the output layer, handling data, specifying a loss function, and training the model. Second, this class of shallow networks happens to comprise the set of linear models, which subsumes many classical methods of statistical prediction, including linear and softmax regression. Understanding these classical tools is pivotal because they are widely used in many contexts and we will often need to use them as baselines when justifying the use of fancier architectures. This chapter will focus narrowly on linear regression and the next one will extend our modeling repertoire by developing linear neural networks for classification.\n\n3.1 Linear Regression\n\nRegression problems pop up whenever we want to predict a numerical value. Common examples include predicting prices (of homes, stocks, etc.), predicting the length of stay (for patients in the hospital), forecasting demand (for retail sales), among numerous others. Not every prediction problem is one of classical regression. Later on, we will introduce classification problems, where the goal is to predict membership among a set of categories.\n\nAs a running example, suppose that we wish to estimate the prices of houses (in dollars) based on their area (in square feet) and age (in years). To develop a model for predicting house prices, we need to get our hands on data, including the sales price, area, and age for each home. In the terminology of machine learning, the dataset is called a training dataset or training set, and each row (containing the data corresponding to one sale) is called an example (or data point, instance, sample). The thing we are trying to predict (price) is called a label (or target). The variables (age and area) upon which the predictions are based are called features (or covariates).\n\n%matplotlib inline\nimport math\nimport time\nimport numpy as np\n\n(continues on next page)"
    },
    "123": {
      "text": "import torch\nfrom d2l import torch as d2l\n\n3.1.1 Basics\n\nLinear regression is both the simplest and most popular among the standard tools for tackling regression problems. Dating back to the dawn of the 19th century (Gauss, 1809, Legendre, 1805), linear regression flows from a few simple assumptions. First, we assume that the relationship between features \\( \\mathbf{x} \\) and target \\( y \\) is approximately linear, i.e., that the conditional mean \\( E[Y | X = x] \\) can be expressed as a weighted sum of the features \\( x \\). This setup allows that the target value may still deviate from its expected value on account of observation noise. Next, we can impose the assumption that any such noise is well behaved, following a Gaussian distribution. Typically, we will use \\( n \\) to denote the number of examples in our dataset. We use superscripts to enumerate samples and targets, and subscripts to index coordinates. More concretely, \\( x^{(i)} \\) denotes the \\( i \\)-th sample and \\( x_j^{(i)} \\) denotes its \\( j \\)-th coordinate.\n\nModel\n\nAt the heart of every solution is a model that describes how features can be transformed into an estimate of the target. The assumption of linearity means that the expected value of the target (price) can be expressed as a weighted sum of the features (area and age):\n\n\\[ \\text{price} = w_{\\text{area}} \\cdot \\text{area} + w_{\\text{age}} \\cdot \\text{age} + b. \\]\n\nHere \\( w_{\\text{area}} \\) and \\( w_{\\text{age}} \\) are called weights, and \\( b \\) is called a bias (or offset or intercept). The weights determine the influence of each feature on our prediction. The bias determines the value of the estimate when all features are zero. Even though we will never see any newly-built homes with precisely zero area, we still need the bias because it allows us to express all linear functions of our features (rather than restricting us to lines that pass through the origin). Strictly speaking, (3.1.1) is an affine transformation of input features, which is characterized by a linear transformation of features via a weighted sum, combined with a translation via the added bias. Given a dataset, our goal is to choose the weights \\( \\mathbf{w} \\) and the bias \\( b \\) that, on average, make our model’s predictions fit the true prices observed in the data as closely as possible.\n\nIn disciplines where it is common to focus on datasets with just a few features, explicitly expressing models long-form, as in (3.1.1), is common. In machine learning, we usually work with high-dimensional datasets, where it is more convenient to employ compact linear algebra notation. When our inputs consist of \\( d \\) features, we can assign each an index (between 1 and \\( d \\)) and express our prediction \\( \\hat{y} \\) (in general the “hat” symbol denotes an estimate) as\n\n\\[ \\hat{y} = w_1 x_1 + \\cdots + w_d x_d + b. \\]"
    },
    "124": {
      "text": "Collecting all features into a vector \\( \\mathbf{x} \\in \\mathbb{R}^d \\) and all weights into a vector \\( \\mathbf{w} \\in \\mathbb{R}^d \\), we can express our model compactly via the dot product between \\( \\mathbf{w} \\) and \\( \\mathbf{x} \\):\n\n\\[\n\\hat{y} = \\mathbf{w}^\\top \\mathbf{x} + b.\n\\]\n\n(3.1.3)\n\nIn (3.1.3), the vector \\( \\mathbf{x} \\) corresponds to the features of a single example. We will often find it convenient to refer to features of our entire dataset of \\( n \\) examples via the design matrix \\( \\mathbf{X} \\in \\mathbb{R}^{n \\times d} \\). Here, \\( \\mathbf{X} \\) contains one row for every example and one column for every feature. For a collection of features \\( \\mathbf{X} \\), the predictions \\( \\hat{\\mathbf{y}} \\in \\mathbb{R}^n \\) can be expressed via the matrix-vector product:\n\n\\[\n\\hat{\\mathbf{y}} = \\mathbf{X} \\mathbf{w} + b,\n\\]\n\n(3.1.4)\n\nwhere broadcasting (Section 2.1.4) is applied during the summation. Given features of a training dataset \\( \\mathbf{X} \\) and corresponding (known) labels \\( \\mathbf{y} \\), the goal of linear regression is to find the weight vector \\( \\mathbf{w} \\) and the bias term \\( b \\) such that, given features of a new data example sampled from the same distribution as \\( \\mathbf{X} \\), the new example’s label will (in expectation) be predicted with the smallest error.\n\nEven if we believe that the best model for predicting \\( y \\) given \\( x \\) is linear, we would not expect to find a real-world dataset of \\( n \\) examples where \\( y^{(i)} \\) exactly equals \\( \\mathbf{w}^\\top \\mathbf{x}^{(i)} + b \\) for all \\( 1 \\leq i \\leq n \\). For example, whatever instruments we use to observe the features \\( \\mathbf{X} \\) and labels \\( \\mathbf{y} \\), there might be a small amount of measurement error. Thus, even when we are confident that the underlying relationship is linear, we will incorporate a noise term to account for such errors.\n\nBefore we can go about searching for the best parameters (or model parameters) \\( \\mathbf{w} \\) and \\( b \\), we will need two more things: (i) a measure of the quality of some given model; and (ii) a procedure for updating the model to improve its quality.\n\n### Loss Function\n\nNaturally, fitting our model to the data requires that we agree on some measure of fitness (or, equivalently, of unfitness). **Loss functions** quantify the distance between the real and predicted values of the target. The loss will usually be a nonnegative number where smaller values are better and perfect predictions incur a loss of 0. For regression problems, the most common loss function is the squared error. When our prediction for an example \\( i \\) is \\( \\hat{y}^{(i)} \\) and the corresponding true label is \\( y^{(i)} \\), the **squared error** is given by:\n\n\\[\nl^{(i)}(\\mathbf{w}, b) = \\frac{1}{2} \\left( \\hat{y}^{(i)} - y^{(i)} \\right)^2.\n\\]\n\n(3.1.5)\n\nThe constant \\( \\frac{1}{2} \\) makes no real difference but proves to be notationally convenient, since it cancels out when we take the derivative of the loss. Because the training dataset is given to us, and thus is out of our control, the empirical error is only a function of the model parameters. In Fig. 3.1.1, we visualize the fit of a linear regression model in a problem with one-dimensional inputs.\n\nNote that large differences between estimates \\( \\hat{y}^{(i)} \\) and targets \\( y^{(i)} \\) lead to even larger contributions to the loss, due to its quadratic form (this quadraticity can be a double-edge sword;"
    },
    "125": {
      "text": "Fitting a linear regression model to one-dimensional data.\n\nwhile it encourages the model to avoid large errors it can also lead to excessive sensitivity to anomalous data). To measure the quality of a model on the entire dataset of \\( n \\) examples, we simply average (or equivalently, sum) the losses on the training set:\n\n\\[\nL(\\mathbf{w}, b) = \\frac{1}{n} \\sum_{i=1}^{n} l^{(i)}(\\mathbf{w}, b) = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{2} \\left( \\mathbf{w}^{\\top} \\mathbf{x}^{(i)} + b - y^{(i)} \\right)^2.\n\\]\n\nWhen training the model, we seek parameters \\((\\mathbf{w}^*, b^*)\\) that minimize the total loss across all training examples:\n\n\\[\n\\mathbf{w}^*, b^* = \\argmin_{\\mathbf{w}, b} L(\\mathbf{w}, b).\n\\]\n\n### Analytic Solution\n\nUnlike most of the models that we will cover, linear regression presents us with a surprisingly easy optimization problem. In particular, we can find the optimal parameters (as assessed on the training data) analytically by applying a simple formula as follows. First, we can subsume the bias \\( b \\) into the parameter \\( \\mathbf{w} \\) by appending a column to the design matrix consisting of all 1s. Then our prediction problem is to minimize \\( \\| \\mathbf{y} - \\mathbf{X} \\mathbf{w} \\| \\). As long as the design matrix \\( \\mathbf{X} \\) has full rank (no feature is linearly dependent on the others), then there will be just one critical point on the loss surface and it corresponds to the minimum of the loss over the entire domain. Taking the derivative of the loss with respect to \\( \\mathbf{w} \\) and setting it equal to zero yields:\n\n\\[\n\\partial_{\\mathbf{w}} \\| \\mathbf{y} - \\mathbf{X} \\mathbf{w} \\| = 2 \\mathbf{X}^{\\top} (\\mathbf{X} \\mathbf{w} - \\mathbf{y}) = 0 \\text{ and hence } \\mathbf{X}^{\\top} \\mathbf{y} = \\mathbf{X}^{\\top} \\mathbf{X} \\mathbf{w}.\n\\]\n\nSolving for \\( \\mathbf{w} \\) provides us with the optimal solution for the optimization problem. Note that this solution\n\n\\[\n\\mathbf{w}^* = (\\mathbf{X}^{\\top} \\mathbf{X})^{-1} \\mathbf{X}^{\\top} \\mathbf{y}\n\\]\n\nwill only be unique when the matrix \\( \\mathbf{X}^{\\top} \\mathbf{X} \\) is invertible, i.e., when the columns of the design matrix are linearly independent (Golub and Van Loan, 1996).\n\nWhile simple problems like linear regression may admit analytic solutions, you should not get used to such good fortune. Although analytic solutions allow for nice mathematical analysis, the requirement of an analytic solution is so restrictive that it would exclude almost all exciting aspects of deep learning."
    },
    "126": {
      "text": "Minibatch Stochastic Gradient Descent\n\nFortunately, even in cases where we cannot solve the models analytically, we can still often train models effectively in practice. Moreover, for many tasks, those hard-to-optimize models turn out to be so much better that figuring out how to train them ends up being well worth the trouble.\n\nThe key technique for optimizing nearly every deep learning model, and which we will call upon throughout this book, consists of iteratively reducing the error by updating the parameters in the direction that incrementally lowers the loss function. This algorithm is called gradient descent.\n\nThe most naive application of gradient descent consists of taking the derivative of the loss function, which is an average of the losses computed on every single example in the dataset. In practice, this can be extremely slow: we must pass over the entire dataset before making a single update, even if the update steps might be very powerful (Liu and Nocedal, 1989). Even worse, if there is a lot of redundancy in the training data, the benefit of a full update is limited.\n\nThe other extreme is to consider only a single example at a time and to take update steps based on one observation at a time. The resulting algorithm, stochastic gradient descent (SGD) can be an effective strategy (Bottou, 2010), even for large datasets. Unfortunately, SGD has drawbacks, both computational and statistical. One problem arises from the fact that processors are a lot faster multiplying and adding numbers than they are at moving data from main memory to processor cache. It is up to an order of magnitude more efficient to perform a matrix–vector multiplication than a corresponding number of vector–vector operations. This means that it can take a lot longer to process one sample at a time compared to a full batch. A second problem is that some of the layers, such as batch normalization (to be described in Section 8.5), only work well when we have access to more than one observation at a time.\n\nThe solution to both problems is to pick an intermediate strategy: rather than taking a full batch or only a single sample at a time, we take a minibatch of observations (Li et al., 2014). The specific choice of the size of the said minibatch depends on many factors, such as the amount of memory, the number of accelerators, the choice of layers, and the total dataset size. Despite all that, a number between 32 and 256, preferably a multiple of a large power of 2, is a good start. This leads us to minibatch stochastic gradient descent.\n\nIn its most basic form, in each iteration \\( t \\), we first randomly sample a minibatch \\( B_t \\) consisting of a fixed number \\( |B| \\) of training examples. We then compute the derivative (gradient) of the average loss on the minibatch with respect to the model parameters. Finally, we multiply the gradient by a predetermined small positive value \\( \\eta \\), called the learning rate, and subtract the resulting term from the current parameter values. We can express the update as follows:\n\n\\[\n(w, b) \\leftarrow (w, b) - \\frac{\\eta}{|B|} \\sum_{i \\in B_t} \\partial_{(w, b)} l^{(i)}(w, b).\n\\]\n\n(3.1.10)\n\nIn summary, minibatch SGD proceeds as follows: (i) initialize the values of the model"
    },
    "127": {
      "text": "parameters, typically at random; (ii) iteratively sample random minibatches from the data, updating the parameters in the direction of the negative gradient. For quadratic losses and affine transformations, this has a closed-form expansion:\n\n\\[\n\\begin{align*}\n\\mathbf{w} &\\leftarrow \\mathbf{w} - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}_t} \\partial_{\\mathbf{w}} l^{(i)}(\\mathbf{w}, b) &= \\mathbf{w} - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}_t} \\mathbf{x}^{(i)} \\left( \\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)} \\right) \\\\\nb &\\leftarrow b - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}_t} \\partial_b l^{(i)}(\\mathbf{w}, b) &= b - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}_t} \\left( \\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)} \\right).\n\\end{align*}\n\\]\n\nSince we pick a minibatch $\\mathcal{B}$ we need to normalize by its size $|\\mathcal{B}|$. Frequently minibatch size and learning rate are user-defined. Such tunable parameters that are not updated in the training loop are called **hyperparameters**. They can be tuned automatically by a number of techniques, such as Bayesian optimization (Frazier, 2018). In the end, the quality of the solution is typically assessed on a separate **validation dataset** (or **validation set**).\n\nAfter training for some predetermined number of iterations (or until some other stopping criterion is met), we record the estimated model parameters, denoted $\\hat{\\mathbf{w}}, \\hat{b}$. Note that even if our function is truly linear and noiseless, these parameters will not be the exact minimizers of the loss, nor even deterministic. Although the algorithm converges slowly towards the minimizers it typically will not find them exactly in a finite number of steps. Moreover, the minibatches $\\mathcal{B}$ used for updating the parameters are chosen at random. This breaks determinism.\n\nLinear regression happens to be a learning problem with a global minimum (whenever $\\mathbf{X}$ is full rank, or equivalently, whenever $\\mathbf{X}^\\top \\mathbf{X}$ is invertible). However, the loss surfaces for deep networks contain many saddle points and minima. Fortunately, we typically do not care about finding an exact set of parameters but merely any set of parameters that leads to accurate predictions (and thus low loss). In practice, deep learning practitioners seldom struggle to find parameters that minimize the loss *on training sets* (Frankle and Carbin, 2018, Izmailov et al., 2018). The more formidable task is to find parameters that lead to accurate predictions on previously unseen data, a challenge called **generalization**. We return to these topics throughout the book.\n\n### Predictions\n\nGiven the model $\\hat{\\mathbf{w}}^\\top \\mathbf{x} + \\hat{b}$, we can now make **predictions** for a new example, e.g., predicting the sales price of a previously unseen house given its area $x_1$ and age $x_2$. Deep learning practitioners have taken to calling the prediction phase **inference** but this is a bit of a misnomer—*inference* refers broadly to any conclusion reached on the basis of evidence, including both the values of the parameters and the likely label for an unseen instance. If anything, in the statistics literature *inference* more often denotes parameter inference and this overloading of terminology creates unnecessary confusion when deep learning practitioners talk to statisticians. In the following we will stick to **prediction** whenever possible.\n\n### 3.1.2 Vectorization for Speed"
    },
    "128": {
      "text": "When training our models, we typically want to process whole minibatches of examples simultaneously. Doing this efficiently requires that we vectorize the calculations and leverage fast linear algebra libraries rather than writing costly for-loops in Python.\n\nTo see why this matters so much, let’s consider two methods for adding vectors. To start, we instantiate two 10,000-dimensional vectors containing all 1s. In the first method, we loop over the vectors with a Python for-loop. In the second, we rely on a single call to +.\n\n```python\nn = 10000\na = torch.ones(n)\nb = torch.ones(n)\n```\n\nNow we can benchmark the workloads. First, we add them, one coordinate at a time, using a for-loop.\n\n```python\nc = torch.zeros(n)\nt = time.time()\nfor i in range(n):\n    c[i] = a[i] + b[i]\nf'{time.time() - t:.5f} sec'\n```\n\n'0.17802 sec'\n\nAlternatively, we rely on the reloaded + operator to compute the elementwise sum.\n\n```python\nt = time.time()\nd = a + b\nf'{time.time() - t:.5f} sec'\n```\n\n'0.00036 sec'\n\nThe second method is dramatically faster than the first. Vectorizing code often yields order-of-magnitude speedups. Moreover, we push more of the mathematics to the library so we do not have to write as many calculations ourselves, reducing the potential for errors and increasing portability of the code.\n\n### 3.1.3 The Normal Distribution and Squared Loss\n\nSo far we have given a fairly functional motivation of the squared loss objective: the optimal parameters return the conditional expectation \\( E[Y \\mid X] \\) whenever the underlying pattern is truly linear, and the loss assigns large penalties for outliers. We can also provide a more formal motivation for the squared loss objective by making probabilistic assumptions about the distribution of noise.\n\nLinear regression was invented at the turn of the 19th century. While it has long been debated whether Gauss or Legendre first thought up the idea, it was Gauss who also discovered the normal distribution (also called the Gaussian). It turns out that the normal"
    },
    "129": {
      "text": "distribution and linear regression with squared loss share a deeper connection than common parentage.\n\nTo begin, recall that a normal distribution with mean $\\mu$ and variance $\\sigma^2$ (standard deviation $\\sigma$) is given as\n\n$$p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{1}{2\\sigma^2}(x - \\mu)^2\\right). \\quad (3.1.12)$$\n\nBelow we define a function to compute the normal distribution.\n\n```python\ndef normal(x, mu, sigma):\n    p = 1 / math.sqrt(2 * math.pi * sigma**2)\n    return p * np.exp(-0.5 * (x - mu)**2 / sigma**2)\n```\n\nWe can now visualize the normal distributions.\n\n```python\n# Use NumPy again for visualization\nx = np.arange(-7, 7, 0.01)\n\n# Mean and standard deviation pairs\nparams = [(0, 1), (0, 2), (3, 1)]\nd2l.plot(x, [normal(x, mu, sigma) for mu, sigma in params], xlabel='x', ylabel='p(x)', figsize=(4.5, 2.5),\n         legend=[f'mean {mu}, std {sigma}' for mu, sigma in params])\n```\n\nNote that changing the mean corresponds to a shift along the $x$-axis, and increasing the variance spreads the distribution out, lowering its peak.\n\nOne way to motivate linear regression with squared loss is to assume that observations arise from noisy measurements, where the noise $\\epsilon$ follows the normal distribution $N(0, \\sigma^2)$:\n\n$$y = w^\\top x + b + \\epsilon \\text{ where } \\epsilon \\sim N(0, \\sigma^2). \\quad (3.1.13)$$\n\nThus, we can now write out the **likelihood** of seeing a particular $y$ for a given $x$ via\n\n$$P(y \\mid x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{1}{2\\sigma^2}(y - w^\\top x - b)^2\\right). \\quad (3.1.14)$$\n\nAs such, the likelihood factorizes. According to **the principle of maximum likelihood**, the"
    },
    "130": {
      "text": "best values of parameters \\( \\mathbf{w} \\) and \\( b \\) are those that maximize the **likelihood** of the entire dataset:\n\n\\[\nP(\\mathbf{y} \\mid \\mathbf{X}) = \\prod_{i=1}^{n} p(y^{(i)} \\mid x^{(i)}).\n\\]\n\n(3.1.15)\n\nThe equality follows since all pairs \\( (x^{(i)}, y^{(i)}) \\) were drawn independently of each other. Estimators chosen according to the principle of maximum likelihood are called **maximum likelihood estimators**. While, maximizing the product of many exponential functions, might look difficult, we can simplify things significantly, without changing the objective, by maximizing the logarithm of the likelihood instead. For historical reasons, optimizations are more often expressed as minimization rather than maximization. So, without changing anything, we can **minimize** the **negative log-likelihood**, which we can express as follows:\n\n\\[\n-\\log P(\\mathbf{y} \\mid \\mathbf{X}) = \\sum_{i=1}^{n} \\frac{1}{2} \\log(2\\pi\\sigma^2) + \\frac{1}{2\\sigma^2} \\left( y^{(i)} - \\mathbf{w}^\\top x^{(i)} - b \\right)^2.\n\\]\n\n(3.1.16)\n\nIf we assume that \\( \\sigma \\) is fixed, we can ignore the first term, because it does not depend on \\( \\mathbf{w} \\) or \\( b \\). The second term is identical to the squared error loss introduced earlier, except for the multiplicative constant \\( \\frac{1}{\\sigma^2} \\). Fortunately, the solution does not depend on \\( \\sigma \\) either. It follows that minimizing the mean squared error is equivalent to the maximum likelihood estimation of a linear model under the assumption of additive Gaussian noise.\n\n### 3.1.4 Linear Regression as a Neural Network\n\nWhile linear models are not sufficiently rich to express the many complicated networks that we will introduce in this book, (artificial) neural networks are rich enough to subsume linear models as networks in which every feature is represented by an input neuron, all of which are connected directly to the output.\n\nFig. 3.1.2 depicts linear regression as a neural network. The diagram highlights the connectivity pattern, such as how each input is connected to the output, but not the specific values taken by the weights or biases.\n\n![Fig. 3.1.2](image)\n\n**Linear regression is a single-layer neural network.**\n\nThe inputs are \\( x_1, \\ldots, x_d \\). We refer to \\( d \\) as the **number of inputs** or the **feature dimensionality** in the input layer. The output of the network is \\( o_1 \\). Because we are just trying to predict a single numerical value, we have only one output neuron. Note that the input values are all **given**. There is just a single **computed** neuron. In summary, we can think of linear regression as a single-layer fully connected neural network. We will encounter networks with far more layers in later chapters."
    },
    "131": {
      "text": "Because linear regression predates computational neuroscience, it might seem anachronistic to describe linear regression in terms of neural networks. Nonetheless, they were a natural place to start when the cyberneticists and neurophysiologists Warren McCulloch and Walter Pitts began to develop models of artificial neurons. Consider the cartoonish picture of a biological neuron in Fig. 3.1.3, consisting of dendrites (input terminals), the nucleus (CPU), the axon (output wire), and the axon terminals (output terminals), enabling connections to other neurons via synapses.\n\nFig. 3.1.3 The real neuron (source: “Anatomy and Physiology” by the US National Cancer Institute’s Surveillance, Epidemiology and End Results (SEER) Program).\n\nInformation \\(x_i\\) arriving from other neurons (or environmental sensors) is received in the dendrites. In particular, that information is weighted by synaptic weights \\(w_i\\), determining the effect of the inputs, e.g., activation or inhibition via the product \\(x_i w_i\\). The weighted inputs arriving from multiple sources are aggregated in the nucleus as a weighted sum \\(y = \\sum_i x_i w_i + b\\), possibly subject to some nonlinear postprocessing via a function \\(\\sigma(y)\\). This information is then sent via the axon to the axon terminals, where it reaches its destination (e.g., an actuator such as a muscle) or it is fed into another neuron via its dendrites.\n\nCertainly, the high-level idea that many such units could be combined, provided they have the correct connectivity and learning algorithm, to produce far more interesting and complex behavior than any one neuron alone could express arises from our study of real biological neural systems. At the same time, most research in deep learning today draws inspiration from a much wider source. We invoke Russell and Norvig (2016) who pointed out that although airplanes might have been inspired by birds, ornithology has not been the primary driver of aeronautics innovation for some centuries. Likewise, inspiration in deep learning these days comes in equal or greater measure from mathematics, linguistics, psychology, statistics, computer science, and many other fields.\n\n3.1.5 Summary\n\nIn this section, we introduced traditional linear regression, where the parameters of a linear function are chosen to minimize squared loss on the training set. We also motivated this choice of objective both via some practical considerations and through an interpretation of linear regression as maximum likelihood estimation under an assumption of linearity and Gaussian noise. After discussing both computational considerations and connections to"
    },
    "132": {
      "text": "statistics, we showed how such linear models could be expressed as simple neural networks where the inputs are directly wired to the output(s). While we will soon move past linear models altogether, they are sufficient to introduce most of the components that all of our models require: parametric forms, differentiable objectives, optimization via minibatch stochastic gradient descent, and ultimately, evaluation on previously unseen data.\n\n### 3.1.6 Exercises\n\n1. Assume that we have some data \\( x_1, \\ldots, x_n \\in \\mathbb{R} \\). Our goal is to find a constant \\( b \\) such that \\( \\sum_i (x_i - b)^2 \\) is minimized.\n   1. Find an analytic solution for the optimal value of \\( b \\).\n   2. How does this problem and its solution relate to the normal distribution?\n   3. What if we change the loss from \\( \\sum_i (x_i - b)^2 \\) to \\( \\sum_i |x_i - b| \\)? Can you find the optimal solution for \\( b \\)?\n\n2. Prove that the affine functions that can be expressed by \\( x^T w + b \\) are equivalent to linear functions on \\( (x, 1) \\).\n\n3. Assume that you want to find quadratic functions of \\( x \\), i.e., \\( f(x) = b + \\sum_i w_i x_i + \\sum_{j \\leq i} w_{ij} x_i x_j \\). How would you formulate this in a deep network?\n\n4. Recall that one of the conditions for the linear regression problem to be solvable was that the design matrix \\( X^T X \\) has full rank.\n   1. What happens if this is not the case?\n   2. How could you fix it? What happens if you add a small amount of coordinate-wise independent Gaussian noise to all entries of \\( X \\)?\n   3. What is the expected value of the design matrix \\( X^T X \\) in this case?\n   4. What happens with stochastic gradient descent when \\( X^T X \\) does not have full rank?\n\n5. Assume that the noise model governing the additive noise \\( \\epsilon \\) is the exponential distribution. That is, \\( p(\\epsilon) = \\frac{1}{2} \\exp(-|\\epsilon|) \\).\n   1. Write out the negative log-likelihood of the data under the model \\( -\\log P(y \\mid X) \\).\n   2. Can you find a closed form solution?\n   3. Suggest a minibatch stochastic gradient descent algorithm to solve this problem. What could possibly go wrong (hint: what happens near the stationary point as we keep on updating the parameters)? Can you fix this?\n\n6. Assume that we want to design a neural network with two layers by composing two linear layers. That is, the output of the first layer becomes the input of the second layer. Why would such a naive composition not work?\n\n7. What happens if you want to use regression for realistic price estimation of houses or stock prices?"
    },
    "133": {
      "text": "1. Show that the additive Gaussian noise assumption is not appropriate. Hint: can we have negative prices? What about fluctuations?\n\n2. Why would regression to the logarithm of the price be much better, i.e., \\( y = \\log(price) \\)?\n\n3. What do you need to worry about when dealing with pennystock, i.e., stock with very low prices? Hint: can you trade at all possible prices? Why is this a bigger problem for cheap stock? For more information review the celebrated Black–Scholes model for option pricing (Black and Scholes, 1973).\n\n8. Suppose we want to use regression to estimate the number of apples sold in a grocery store.\n\n1. What are the problems with a Gaussian additive noise model? Hint: you are selling apples, not oil.\n\n2. The Poisson distribution\\(^{68}\\) captures distributions over counts. It is given by \\( p(k | \\lambda) = \\lambda^k e^{-\\lambda} / k! \\). Here \\( \\lambda \\) is the rate function and \\( k \\) is the number of events you see. Prove that \\( \\lambda \\) is the expected value of counts \\( k \\).\n\n3. Design a loss function associated with the Poisson distribution.\n\n4. Design a loss function for estimating \\( \\log \\lambda \\) instead.\n\nDiscussions\\(^{69}\\).\n\n3.2 Object-Oriented Design for Implementation\n\nIn our introduction to linear regression, we walked through various components including the data, the model, the loss function, and the optimization algorithm. Indeed, linear regression is one of the simplest machine learning models. Training it, however, uses many of the same components that other models in this book require. Therefore, before diving into the implementation details it is worth designing some of the APIs that we use throughout. Treating components in deep learning as objects, we can start by defining classes for these objects and their interactions. This object-oriented design for implementation will greatly streamline the presentation and you might even want to use it in your projects.\n\nInspired by open-source libraries such as PyTorch Lightning\\(^{70}\\), at a high level we wish to have three classes: (i) Module contains models, losses, and optimization methods; (ii) DataModule provides data loaders for training and validation; (iii) both classes are combined using the Trainer class, which allows us to train models on a variety of hardware platforms. Most code in this book adapts Module and DataModule. We will touch upon the Trainer class only when we discuss GPUs, CPUs, parallel training, and optimization algorithms."
    },
    "134": {
      "text": "import time\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\n### 3.2.1 Utilities\n\nWe need a few utilities to simplify object-oriented programming in Jupyter notebooks. One of the challenges is that class definitions tend to be fairly long blocks of code. Notebook readability demands short code fragments, interspersed with explanations, a requirement incompatible with the style of programming common for Python libraries. The first utility function allows us to register functions as methods in a class *after* the class has been created. In fact, we can do so *even after* we have created instances of the class! It allows us to split the implementation of a class into multiple code blocks.\n\n```python\ndef add_to_class(Class):\n    # @save\n    \"\"\"Register functions as methods in created class.\"\"\"\n    def wrapper(obj):\n        setattr(Class, obj.__name__, obj)\n    return wrapper\n```\n\nLet’s have a quick look at how to use it. We plan to implement a class A with a method `do`. Instead of having code for both A and do in the same code block, we can first declare the class A and create an instance `a`.\n\n```python\nclass A:\n    def __init__(self):\n        self.b = 1\n\na = A()\n```\n\nNext we define the method `do` as we normally would, but not in class A’s scope. Instead, we decorate this method by `add_to_class` with class A as its argument. In doing so, the method is able to access the member variables of A just as we would expect had it been included as part of A’s definition. Let’s see what happens when we invoke it for the instance `a`.\n\n```python\n@add_to_class(A)\ndef do(self):\n    print('Class attribute \"b\" is', self.b)\n\na.do()\n```\n\nClass attribute “b” is 1\n\nThe second one is a utility class that saves all arguments in a class’s `__init__` method"
    },
    "135": {
      "text": "as class attributes. This allows us to extend constructor call signatures implicitly without additional code.\n\n```python\nclass HyperParameters:  #@save\n    \"\"\"The base class of hyperparameters.\"\"\"\n    def save_hyperparameters(self, ignore=[]):\n        raise NotImplementedError\n```\n\nWe defer its implementation into Section B.7. To use it, we define our class that inherits from `HyperParameters` and calls `save_hyperparameters` in the `__init__` method.\n\n```python\n# Call the fully implemented HyperParameters class saved in d2l\nclass B(d2l.HyperParameters):\n    def __init__(self, a, b, c):\n        self.save_hyperparameters(ignore=['c'])\n        print('self.a =', self.a, 'self.b =', self.b)\n        print('There is no self.c =', not hasattr(self, 'c'))\n\nb = B(a=1, b=2, c=3)\n```\n\n```python\nself.a = 1 self.b = 2\nThere is no self.c = True\n```\n\nThe final utility allows us to plot experiment progress interactively while it is going on. In deference to the much more powerful (and complex) TensorBoard\\(^{71}\\) we name it ProgressBoard. The implementation is deferred to Section B.7. For now, let’s simply see it in action.\n\nThe `draw` method plots a point \\((x, y)\\) in the figure, with label specified in the legend. The optional `every_n` smooths the line by only showing \\(1/n\\) points in the figure. Their values are averaged from the \\(n\\) neighbor points in the original figure.\n\n```python\nclass ProgressBoard(d2l.HyperParameters):  #@save\n    \"\"\"The board that plots data points in animation.\"\"\"\n    def __init__(self, xlabel=None, ylabel=None, xlim=None,\n                 ylim=None, xscale='linear', yscale='linear',\n                 ls=['-', '--', '-', ':'], colors=['C0', 'C1', 'C2', 'C3'],\n                 fig=None, axes=None, figsize=(3.5, 2.5), display=True):\n        self.save_hyperparameters()\n\n    def draw(self, x, y, label, every_n=1):\n        raise NotImplementedError\n```\n\nIn the following example, we draw \\(\\sin\\) and \\(\\cos\\) with a different smoothness. If you run this code block, you will see the lines grow in animation.\n\n```python\nboard = d2l.ProgressBoard('x')\nfor x in np.arange(0, 10, 0.1):\n    board.draw(x, np.sin(x), 'sin', every_n=2)\n    board.draw(x, np.cos(x), 'cos', every_n=10)\n```"
    },
    "136": {
      "text": "The Module class is the base class of all models we will implement. At the very least we need three methods. The first, __init__, stores the learnable parameters, the training_step method accepts a data batch to return the loss value, and finally, configure_optimizers returns the optimization method, or a list of them, that is used to update the learnable parameters. Optionally we can define validation_step to report the evaluation measures. Sometimes we put the code for computing the output into a separate forward method to make it more reusable.\n\n```python\nclass Module(nn.Module, d2l.HyperParameters):  #@save\n    \"\"\"The base class of models.\"\"\"\n    def __init__(self, plot_train_per_epoch=2, plot_valid_per_epoch=1):\n        super().__init__()\n        self.save_hyperparameters()\n        self.board = ProgressBoard()\n\n    def loss(self, y_hat, y):\n        raise NotImplementedError\n\n    def forward(self, X):\n        assert hasattr(self, 'net'), 'Neural network is defined'\n        return self.net(X)\n\n    def plot(self, key, value, train):\n        \"\"\"Plot a point in animation.\"\"\"\n        assert hasattr(self, 'trainer'), 'Trainer is not inited'\n        self.board.xlabel = 'epoch'\n        if train:\n            x = self.trainer.train_batch_idx / \\\n                self.trainer.num_train_batches\n            n = self.trainer.num_train_batches / \\\n                self.plot_train_per_epoch\n        else:\n            x = self.trainer.epoch + 1\n            n = self.trainer.num_val_batches / \\\n                self.plot_valid_per_epoch\n        self.board.draw(x, value.to(d2l.cpu()).detach().numpy(),\n                        ('train_' if train else 'val_') + key,\n                        every_n=int(n))\n```\n\n(continues on next page)"
    },
    "137": {
      "text": "(continued from previous page)\n\n```python\ndef training_step(self, batch):\n    l = self.loss(self(*batch[:-1]), batch[-1])\n    self.plot('loss', l, train=True)\n    return l\n\ndef validation_step(self, batch):\n    l = self.loss(self(*batch[:-1]), batch[-1])\n    self.plot('loss', l, train=False)\n\ndef configure_optimizers(self):\n    raise NotImplementedError\n```\n\nYou may notice that `Module` is a subclass of `nn.Module`, the base class of neural networks in PyTorch. It provides convenient features for handling neural networks. For example, if we define a forward method, such as `forward(self, X)`, then for an instance `a` we can invoke this method by `a(X)`. This works since it calls the forward method in the built-in `__call__` method. You can find more details and examples about `nn.Module` in Section 6.1.\n\n### 3.2.3 Data\n\nThe `DataModule` class is the base class for data. Quite frequently the `__init__` method is used to prepare the data. This includes downloading and preprocessing if needed. The `train_dataloader` returns the data loader for the training dataset. A data loader is a (Python) generator that yields a data batch each time it is used. This batch is then fed into the `training_step` method of `Module` to compute the loss. There is an optional `val_dataloader` to return the validation dataset loader. It behaves in the same manner, except that it yields data batches for the `validation_step` method in `Module`.\n\n```python\nclass DataModule(d2l.HyperParameters):  # @save\n    \"\"\"The base class of data.\"\"\"\n    def __init__(self, root='../data', num_workers=4):\n        self.save_hyperparameters()\n\n    def get_dataloader(self, train):\n        raise NotImplementedError\n\n    def train_dataloader(self):\n        return self.get_dataloader(train=True)\n\n    def val_dataloader(self):\n        return self.get_dataloader(train=False)\n```\n\n### 3.2.4 Training\n\nThe `Trainer` class trains the learnable parameters in the `Module` class with data specified in `DataModule`. The key method is `fit`, which accepts two arguments: `model`, an instance of `Module`, and `data`, an instance of `DataModule`. It then iterates over the entire dataset"
    },
    "138": {
      "text": "max_epochs times to train the model. As before, we will defer the implementation of this method to later chapters.\n\n```python\nclass Trainer(d2l.HyperParameters):\n    \"\"\"The base class for training models with data.\"\"\"\n    def __init__(self, max_epochs, num_gpus=0, gradient_clip_val=0):\n        self.save_hyperparameters()\n        assert num_gpus == 0, 'No GPU support yet'\n\n    def prepare_data(self, data):\n        self.train_dataloader = data.train_dataloader()\n        self.val_dataloader = data.val_dataloader()\n        self.num_train_batches = len(self.train_dataloader)\n        self.num_val_batches = (len(self.val_dataloader) if self.val_dataloader is not None else 0)\n\n    def prepare_model(self, model):\n        model.trainer = self\n        model.board.xlim = [0, self.max_epochs]\n        self.model = model\n\n    def fit(self, model, data):\n        self.prepare_data(data)\n        self.prepare_model(model)\n        self.optim = model.configure_optimizers()\n        self.epoch = 0\n        self.train_batch_idx = 0\n        self.val_batch_idx = 0\n        for self.epoch in range(self.max_epochs):\n            self.fit_epoch()\n\n    def fit_epoch(self):\n        raise NotImplementedError\n```\n\n3.2.5 Summary\n\nTo highlight the object-oriented design for our future deep learning implementation, the above classes simply show how their objects store data and interact with each other. We will keep enriching implementations of these classes, such as via @add_to_class, in the rest of the book. Moreover, these fully implemented classes are saved in the D2L library\\(^{72}\\), a lightweight toolkit that makes structured modeling for deep learning easy. In particular, it facilitates reusing many components between projects without changing much at all. For instance, we can replace just the optimizer, just the model, just the dataset, etc.; this degree of modularity pays dividends throughout the book in terms of conciseness and simplicity (this is why we added it) and it can do the same for your own projects.\n\n3.2.6 Exercises\n\n1. Locate full implementations of the above classes that are saved in the D2L library\\(^{73}\\). We strongly recommend that you look at the implementation in detail once you have gained some more familiarity with deep learning modeling."
    },
    "139": {
      "text": "2. Remove the `save_hyperparameters` statement in the B class. Can you still print `self.a` and `self.b`? Optional: if you have dived into the full implementation of the `HyperParameters` class, can you explain why?\n\nDiscussions\\(^{74}\\).\n\n3.3 Synthetic Regression Data\n\nMachine learning is all about extracting information from data. So you might wonder, what could we possibly learn from synthetic data? While we might not care intrinsically about the patterns that we ourselves baked into an artificial data generating model, such datasets are nevertheless useful for didactic purposes, helping us to evaluate the properties of our learning algorithms and to confirm that our implementations work as expected. For example, if we create data for which the correct parameters are known *a priori*, then we can check that our model can in fact recover them.\n\n```python\n%matplotlib inline\nimport random\nimport torch\nfrom d2l import torch as d2l\n```\n\n3.3.1 Generating the Dataset\n\nFor this example, we will work in low dimension for succinctness. The following code snippet generates 1000 examples with 2-dimensional features drawn from a standard normal distribution. The resulting design matrix \\(X\\) belongs to \\(\\mathbb{R}^{1000 \\times 2}\\). We generate each label by applying a *ground truth* linear function, corrupting them via additive noise \\(\\epsilon\\), drawn independently and identically for each example:\n\n\\[y = Xw + b + \\epsilon.\\]\n\n(3.3.1)\n\nFor convenience we assume that \\(\\epsilon\\) is drawn from a normal distribution with mean \\(\\mu = 0\\) and standard deviation \\(\\sigma = 0.01\\). Note that for object-oriented design we add the code to the `__init__` method of a subclass of d2l.DataModule (introduced in Section 3.2.3). It is good practice to allow the setting of any additional hyperparameters. We accomplish this with `save_hyperparameters()`. The batch_size will be determined later.\n\n```python\nclass SyntheticRegressionData(d2l.DataModule):  # @save\n    \"\"\"Synthetic data for linear regression.\"\"\"\n    def __init__(self, w, b, noise=0.01, num_train=1000, num_val=1000,\n                 batch_size=32):\n        super().__init__()\n        self.save_hyperparameters()\n        n = num_train + num_val\n```\n\n(continues on next page)"
    },
    "140": {
      "text": "(self.X = torch.randn(n, len(w))\nnoise = torch.randn(n, 1) * noise\nself.y = torch.matmul(self.X, w.reshape((-1, 1))) + b + noise\n\nBelow, we set the true parameters to w = [2, -3.4] and b = 4.2. Later, we can check our estimated parameters against these ground truth values.\n\ndata = SyntheticRegressionData(w=torch.tensor([2, -3.4]), b=4.2)\n\nEach row in features consists of a vector in R^2 and each row in labels is a scalar. Let’s have a look at the first entry.\n\nprint('features:', data.X[0], '\\nlabel:', data.y[0])\n\nfeatures: tensor([0.9026, 1.0264])\nlabel: tensor([2.5148])\n\n3.3.2 Reading the Dataset\n\nTraining machine learning models often requires multiple passes over a dataset, grabbing one minibatch of examples at a time. This data is then used to update the model. To illustrate how this works, we implement the get_dataloader method, registering it in the SyntheticRegressionData class via add_to_class (introduced in Section 3.2.1). It takes a batch size, a matrix of features, and a vector of labels, and generates minibatches of size batch_size. As such, each minibatch consists of a tuple of features and labels. Note that we need to be mindful of whether we’re in training or validation mode: in the former, we will want to read the data in random order, whereas for the latter, being able to read data in a pre-defined order may be important for debugging purposes.\n\n@d2l.add_to_class(SyntheticRegressionData)\ndef get_dataloader(self, train):\n    if train:\n        indices = list(range(0, self.num_train))\n        # The examples are read in random order\n        random.shuffle(indices)\n    else:\n        indices = list(range(self.num_train, self.num_train + self.num_val))\n        for i in range(0, len(indices), self.batch_size):\n            batch_indices = torch.tensor(indices[i:i + self.batch_size])\n            yield self.X[batch_indices], self.y[batch_indices]\n\nTo build some intuition, let’s inspect the first minibatch of data. Each minibatch of features provides us with both its size and the dimensionality of input features. Likewise, our minibatch of labels will have a matching shape given by batch_size."
    },
    "141": {
      "text": "X, y = next(iter(data.train_dataloader()))\nprint('X shape:', X.shape, '\\ny shape:', y.shape)\n\nX shape: torch.Size([32, 2])\ny shape: torch.Size([32, 1])\n\nWhile seemingly innocuous, the invocation of iter(data.train_dataloader()) illustrates the power of Python’s object-oriented design. Note that we added a method to the SyntheticRegressionData class after creating the data object. Nonetheless, the object benefits from the ex post facto addition of functionality to the class.\n\nThroughout the iteration we obtain distinct minibatches until the entire dataset has been exhausted (try this). While the iteration implemented above is good for didactic purposes, it is inefficient in ways that might get us into trouble with real problems. For example, it requires that we load all the data in memory and that we perform lots of random memory access. The built-in iterators implemented in a deep learning framework are considerably more efficient and they can deal with sources such as data stored in files, data received via a stream, and data generated or processed on the fly. Next let’s try to implement the same method using built-in iterators.\n\n3.3.3 Concise Implementation of the Data Loader\n\nRather than writing our own iterator, we can call the existing API in a framework to load data. As before, we need a dataset with features X and labels y. Beyond that, we set batch_size in the built-in data loader and let it take care of shuffling examples efficiently.\n\n@d2l.add_to_class(d2l.DataModule)  #@save\ndef get_tensorloader(self, tensors, train, indices=slice(0, None)):\n    tensors = tuple(a[indices] for a in tensors)\n    dataset = torch.utils.data.TensorDataset(*tensors)\n    return torch.utils.data.DataLoader(dataset, self.batch_size,\n                                      shuffle=train)\n\n@d2l.add_to_class(SyntheticRegressionData)  #@save\ndef get_data_loader(self, train):\n    i = slice(0, self.num_train) if train else slice(self.num_train, None)\n    return self.get_tensorloader((self.X, self.y), train, i)\n\nThe new data loader behaves just like the previous one, except that it is more efficient and has some added functionality.\n\nX, y = next(iter(data.train_dataloader()))\nprint('X shape:', X.shape, '\\ny shape:', y.shape)"
    },
    "142": {
      "text": "X shape: torch.Size([32, 2])\ny shape: torch.Size([32, 1])\n\nFor instance, the data loader provided by the framework API supports the built-in __len__ method, so we can query its length, i.e., the number of batches.\n\nlen(data.train_dataloader())\n\n32\n\n3.3.4 Summary\n\nData loaders are a convenient way of abstracting out the process of loading and manipulating data. This way the same machine learning algorithm is capable of processing many different types and sources of data without the need for modification. One of the nice things about data loaders is that they can be composed. For instance, we might be loading images and then have a postprocessing filter that crops them or modifies them in other ways. As such, data loaders can be used to describe an entire data processing pipeline.\n\nAs for the model itself, the two-dimensional linear model is about the simplest we might encounter. It lets us test out the accuracy of regression models without worrying about having insufficient amounts of data or an underdetermined system of equations. We will put this to good use in the next section.\n\n3.3.5 Exercises\n\n1. What will happen if the number of examples cannot be divided by the batch size. How would you change this behavior by specifying a different argument by using the framework’s API?\n\n2. Suppose that we want to generate a huge dataset, where both the size of the parameter vector w and the number of examples num_examples are large.\n\n   1. What happens if we cannot hold all data in memory?\n\n   2. How would you shuffle the data if it is held on disk? Your task is to design an efficient algorithm that does not require too many random reads or writes. Hint: pseudorandom permutation generators allow you to design a reshuffle without the need to store the permutation table explicitly (Naor and Reingold, 1999).\n\n3. Implement a data generator that produces new data on the fly, every time the iterator is called.\n\n4. How would you design a random data generator that generates the same data each time it is called?\n\nDiscussions"
    },
    "143": {
      "text": "3.4 Linear Regression Implementation from Scratch\n\nWe are now ready to work through a fully functioning implementation of linear regression. In this section, we will implement the entire method from scratch, including (i) the model; (ii) the loss function; (iii) a minibatch stochastic gradient descent optimizer; and (iv) the training function that stitches all of these pieces together. Finally, we will run our synthetic data generator from Section 3.3 and apply our model on the resulting dataset. While modern deep learning frameworks can automate nearly all of this work, implementing things from scratch is the only way to make sure that you really know what you are doing. Moreover, when it is time to customize models, defining our own layers or loss functions, understanding how things work under the hood will prove handy. In this section, we will rely only on tensors and automatic differentiation. Later, we will introduce a more concise implementation, taking advantage of the bells and whistles of deep learning frameworks while retaining the structure of what follows below.\n\n%matplotlib inline\nimport torch\nfrom d2l import torch as d2l\n\n3.4.1 Defining the Model\n\nBefore we can begin optimizing our model’s parameters by minibatch SGD, we need to have some parameters in the first place. In the following we initialize weights by drawing random numbers from a normal distribution with mean 0 and a standard deviation of 0.01. The magic number 0.01 often works well in practice, but you can specify a different value through the argument sigma. Moreover we set the bias to 0. Note that for object-oriented design we add the code to the __init__ method of a subclass of d2l.Module (introduced in Section 3.2.2).\n\nclass LinearRegressionScratch(d2l.Module):  #@save\n    \"\"\"The linear regression model implemented from scratch.\"\"\"\n    def __init__(self, num_inputs, lr, sigma=0.01):\n        super().__init__()\n        self.save_hyperparameters()\n        self.w = torch.normal(0, sigma, (num_inputs, 1), requires_grad=True)\n        self.b = torch.zeros(1, requires_grad=True)\n\nNext we must define our model, relating its input and parameters to its output. Using the same notation as (3.1.4) for our linear model we simply take the matrix–vector product of the input features X and the model weights w, and add the offset b to each example. The product Xw is a vector and b is a scalar. Because of the broadcasting mechanism (see Section 2.1.4), when we add a vector and a scalar, the scalar is added to each component of the vector. The resulting forward method is registered in the LinearRegressionScratch class via add_to_class (introduced in Section 3.2.1)."
    },
    "144": {
      "text": "3.4.2 Defining the Loss Function\n\nSince updating our model requires taking the gradient of our loss function, we ought to define the loss function first. Here we use the squared loss function in (3.1.5). In the implementation, we need to transform the true value y into the predicted value’s shape y_hat. The result returned by the following method will also have the same shape as y_hat. We also return the averaged loss value among all examples in the minibatch.\n\n3.4.3 Defining the Optimization Algorithm\n\nAs discussed in Section 3.1, linear regression has a closed-form solution. However, our goal here is to illustrate how to train more general neural networks, and that requires that we teach you how to use minibatch SGD. Hence we will take this opportunity to introduce your first working example of SGD. At each step, using a minibatch randomly drawn from our dataset, we estimate the gradient of the loss with respect to the parameters. Next, we update the parameters in the direction that may reduce the loss.\n\nThe following code applies the update, given a set of parameters, a learning rate lr. Since our loss is computed as an average over the minibatch, we do not need to adjust the learning rate against the batch size. In later chapters we will investigate how learning rates should be adjusted for very large minibatches as they arise in distributed large-scale learning. For now, we can ignore this dependency.\n\nWe define our SGD class, a subclass of d2l.HyperParameters (introduced in Section 3.2.1), to have a similar API as the built-in SGD optimizer. We update the parameters in the step method. The zero_grad method sets all gradients to 0, which must be run before a back-propagation step.\n\nclass SGD(d2l.HyperParameters):  #@save\n    \"\"\"Minibatch stochastic gradient descent.\"\"\"\n    def __init__(self, params, lr):\n        self.save_hyperparameters()\n\n    def step(self):\n        for param in self.params:\n            param -= self.lr * param.grad\n\n    def zero_grad(self):\n(continues on next page)"
    },
    "145": {
      "text": "(continued from previous page)\n\n```python\nfor param in self.params:\n    if param.grad is not None:\n        param.grad.zero_()\n```\n\nWe next define the `configure_optimizers` method, which returns an instance of the SGD class.\n\n@d2l.add_to_class(LinearRegressionScratch)  #@save\ndef configure_optimizers(self):\n    return SGD([self.w, self.b], self.lr)\n```\n\n3.4.4 Training\n\nNow that we have all of the parts in place (parameters, loss function, model, and optimizer), we are ready to implement the main training loop. It is crucial that you understand this code fully since you will employ similar training loops for every other deep learning model covered in this book. In each *epoch*, we iterate through the entire training dataset, passing once through every example (assuming that the number of examples is divisible by the batch size). In each *iteration*, we grab a minibatch of training examples, and compute its loss through the model’s `training_step` method. Then we compute the gradients with respect to each parameter. Finally, we will call the optimization algorithm to update the model parameters. In summary, we will execute the following loop:\n\n- Initialize parameters $(\\mathbf{w}, b)$\n- Repeat until done\n  - Compute gradient $\\mathbf{g} \\leftarrow \\partial_{(\\mathbf{w}, b)} \\frac{1}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} l(\\mathbf{x}^{(i)}, y^{(i)}, \\mathbf{w}, b)$\n  - Update parameters $(\\mathbf{w}, b) \\leftarrow (\\mathbf{w}, b) - \\eta \\mathbf{g}$\n\nRecall that the synthetic regression dataset that we generated in Section 3.3 does not provide a validation dataset. In most cases, however, we will want a validation dataset to measure our model quality. Here we pass the validation dataloader once in each epoch to measure the model performance. Following our object-oriented design, the `prepare_batch` and `fit_epoch` methods are registered in the d2l.Trainer class (introduced in Section 3.2.4).\n\n@d2l.add_to_class(d2l.Trainer)  #@save\ndef prepare_batch(self, batch):\n    return batch\n\n@d2l.add_to_class(d2l.Trainer)  #@save\ndef fit_epoch(self):\n    self.model.train()\n    for batch in self.train_dataloader:\n        loss = self.model.training_step(self.prepare_batch(batch))\n```\n(continues on next page)"
    },
    "146": {
      "text": "We are almost ready to train the model, but first we need some training data. Here we use the SyntheticRegressionData class and pass in some ground truth parameters. Then we train our model with the learning rate lr=0.03 and set max_epochs=3. Note that in general, both the number of epochs and the learning rate are hyperparameters. In general, setting hyperparameters is tricky and we will usually want to use a three-way split, one set for training, a second for hyperparameter selection, and the third reserved for the final evaluation. We elide these details for now but will revise them later.\n\n```python\nmodel = LinearRegressionScratch(2, lr=0.03)\ndata = d2l.SyntheticRegressionData(w=torch.tensor([2, -3.4]), b=4.2)\ntrainer = d2l.Trainer(max_epochs=3)\ntrainer.fit(model, data)\n```\n\nBecause we synthesized the dataset ourselves, we know precisely what the true parameters are. Thus, we can evaluate our success in training by comparing the true parameters with those that we learned through our training loop. Indeed they turn out to be very close to each other.\n\n```python\nwith torch.no_grad():\n    print(f'error in estimating w: {data.w - model.w.reshape(data.w.shape)}')\n    print(f'error in estimating b: {data.b - model.b}')\n```"
    },
    "147": {
      "text": "error in estimating w: tensor([0.1408, -0.1493])\nerror in estimating b: tensor([0.2130])\n\nWe should not take the ability to exactly recover the ground truth parameters for granted. In general, for deep models unique solutions for the parameters do not exist, and even for linear models, exactly recovering the parameters is only possible when no feature is linearly dependent on the others. However, in machine learning, we are often less concerned with recovering true underlying parameters, but rather with parameters that lead to highly accurate prediction (Vapnik, 1992). Fortunately, even on difficult optimization problems, stochastic gradient descent can often find remarkably good solutions, owing partly to the fact that, for deep networks, there exist many configurations of the parameters that lead to highly accurate prediction.\n\n3.4.5 Summary\n\nIn this section, we took a significant step towards designing deep learning systems by implementing a fully functional neural network model and training loop. In this process, we built a data loader, a model, a loss function, an optimization procedure, and a visualization and monitoring tool. We did this by composing a Python object that contains all relevant components for training a model. While this is not yet a professional-grade implementation it is perfectly functional and code like this could already help you to solve small problems quickly. In the coming sections, we will see how to do this both more concisely (avoiding boilerplate code) and more efficiently (using our GPUs to their full potential).\n\n3.4.6 Exercises\n\n1. What would happen if we were to initialize the weights to zero. Would the algorithm still work? What if we initialized the parameters with variance 1000 rather than 0.01?\n\n2. Assume that you are Georg Simon Ohm trying to come up with a model for resistance that relates voltage and current. Can you use automatic differentiation to learn the parameters of your model?\n\n3. Can you use Planck’s Law to determine the temperature of an object using spectral energy density? For reference, the spectral density \\( B(\\lambda, T) = \\frac{2hc^2}{\\lambda^5} \\cdot \\left( \\exp \\frac{hc}{\\lambda kT} - 1 \\right)^{-1} \\). Here \\(\\lambda\\) is the wavelength, \\(T\\) is the temperature, \\(c\\) is the speed of light, \\(h\\) is Planck’s constant, and \\(k\\) is the Boltzmann constant. You measure the energy for different wavelengths \\(\\lambda\\) and you now need to fit the spectral density curve to Planck’s law.\n\n4. What are the problems you might encounter if you wanted to compute the second derivatives of the loss? How would you fix them?\n\n5. Why is the reshape method needed in the loss function?\n\n6. Experiment using different learning rates to find out how quickly the loss function value drops. Can you reduce the error by increasing the number of epochs of training?"
    },
    "148": {
      "text": "7. If the number of examples cannot be divided by the batch size, what happens to data_iter at the end of an epoch?\n\n8. Try implementing a different loss function, such as the absolute value loss (y_hat - d2l.reshape(y, y_hat.shape)).abs().sum().\n\n1. Check what happens for regular data.\n\n2. Check whether there is a difference in behavior if you actively perturb some entries, such as \\( y_5 = 10000 \\), of \\( y \\).\n\n3. Can you think of a cheap solution for combining the best aspects of squared loss and absolute value loss? Hint: how can you avoid really large gradient values?\n\n9. Why do we need to reshuffle the dataset? Can you design a case where a maliciously constructed dataset would break the optimization algorithm otherwise?\n\nDiscussions \\(^{79}\\).\n\n3.5 Concise Implementation of Linear Regression\n\nDeep learning has witnessed a sort of Cambrian explosion over the past decade. The sheer number of techniques, applications and algorithms by far surpasses the progress of previous decades. This is due to a fortuitous combination of multiple factors, one of which is the powerful free tools offered by a number of open-source deep learning frameworks. Theano (Bergstra et al., 2010), DistBelief (Dean et al., 2012), and Caffe (Jia et al., 2014) arguably represent the first generation of such models that found widespread adoption. In contrast to earlier (seminal) works like SN2 (Simulateur Neuristique) (Bottou and Le Cun, 1988), which provided a Lisp-like programming experience, modern frameworks offer automatic differentiation and the convenience of Python. These frameworks allow us to automate and modularize the repetitive work of implementing gradient-based learning algorithms.\n\nIn Section 3.4, we relied only on (i) tensors for data storage and linear algebra; and (ii) automatic differentiation for calculating gradients. In practice, because data iterators, loss functions, optimizers, and neural network layers are so common, modern libraries implement these components for us as well. In this section, we will show you how to implement the linear regression model from Section 3.4 concisely by using high-level APIs of deep learning frameworks.\n\n```python\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n```"
    },
    "149": {
      "text": "3.5.1 Defining the Model\n\nWhen we implemented linear regression from scratch in Section 3.4, we defined our model parameters explicitly and coded up the calculations to produce output using basic linear algebra operations. You should know how to do this. But once your models get more complex, and once you have to do this nearly every day, you will be glad of the assistance. The situation is similar to coding up your own blog from scratch. Doing it once or twice is rewarding and instructive, but you would be a lousy web developer if you spent a month reinventing the wheel.\n\nFor standard operations, we can use a framework’s predefined layers, which allow us to focus on the layers used to construct the model rather than worrying about their implementation. Recall the architecture of a single-layer network as described in Fig. 3.1.2. The layer is called fully connected, since each of its inputs is connected to each of its outputs by means of a matrix-vector multiplication.\n\nIn PyTorch, the fully connected layer is defined in Linear and LazyLinear classes (available since version 1.8.0). The latter allows users to specify merely the output dimension, while the former additionally asks for how many inputs go into this layer. Specifying input shapes is inconvenient and may require nontrivial calculations (such as in convolutional layers). Thus, for simplicity, we will use such “lazy” layers whenever we can.\n\n```python\nclass LinearRegression(d2l.Module):\n    \"\"\"The linear regression model implemented with high-level APIs.\"\"\"\n    def __init__(self, lr):\n        super().__init__()\n        self.save_hyperparameters()\n        self.net = nn.LazyLinear(1)\n        self.net.weight.data.normal_(0, 0.01)\n        self.net.bias.data.fill_(0)\n```\n\nIn the forward method we just invoke the built-in __call__ method of the predefined layers to compute the outputs.\n\n@d2l.add_to_class(LinearRegression) #@save\ndef forward(self, X):\n    return self.net(X)\n\n3.5.2 Defining the Loss Function\n\nThe MSELoss class computes the mean squared error (without the 1/2 factor in (3.1.5)). By default, MSELoss returns the average loss over examples. It is faster (and easier to use) than implementing our own.\n\n@d2l.add_to_class(LinearRegression) #@save\ndef loss(self, y_hat, y):\n    fn = nn.MSELoss()\n    return fn(y_hat, y)"
    },
    "150": {
      "text": "3.5.3 Defining the Optimization Algorithm\n\nMinibatch SGD is a standard tool for optimizing neural networks and thus PyTorch supports it alongside a number of variations on this algorithm in the optim module. When we instantiate an SGD instance, we specify the parameters to optimize over, obtainable from our model via self.parameters(), and the learning rate (self.lr) required by our optimization algorithm.\n\n@d2l.add_to_class(LinearRegression)  #@save\ndef configure_optimizers(self):\n    return torch.optim.SGD(self.parameters(), self.lr)\n\n3.5.4 Training\n\nYou might have noticed that expressing our model through high-level APIs of a deep learning framework requires fewer lines of code. We did not have to allocate parameters individually, define our loss function, or implement minibatch SGD. Once we start working with much more complex models, the advantages of the high-level API will grow considerably.\n\nNow that we have all the basic pieces in place, the training loop itself is the same as the one we implemented from scratch. So we just call the fit method (introduced in Section 3.2.4), which relies on the implementation of the fit_epoch method in Section 3.4, to train our model.\n\nmodel = LinearRegression(lr=0.03)\ndata = d2l.SyntheticRegressionData(w=torch.tensor([2, -3.4]), b=4.2)\ntrainer = d2l.Trainer(max_epochs=3)\ntrainer.fit(model, data)\n\nBelow, we compare the model parameters learned by training on finite data and the actual parameters that generated our dataset. To access parameters, we access the weights and bias of the layer that we need. As in our implementation from scratch, note that our estimated parameters are close to their true counterparts.\n\n@d2l.add_to_class(LinearRegression)  #@save\n(continues on next page)"
    },
    "151": {
      "text": "(continued from previous page)\n\n```python\ndef get_w_b(self):\n    return (self.net.weight.data, self.net.bias.data)\nw, b = model.get_w_b()\n```\n\n```python\nprint(f'error in estimating w: {data.w - w.reshape(data.w.shape)}')\nprint(f'error in estimating b: {data.b - b}')\n```\n\n```python\nerror in estimating w: tensor([0.0094, -0.0030])\nerror in estimating b: tensor([0.0137])\n```\n\n### 3.5.5 Summary\n\nThis section contains the first implementation of a deep network (in this book) to tap into the conveniences afforded by modern deep learning frameworks, such as MXNet (Chen et al., 2015), JAX (Frostig et al., 2018), PyTorch (Paszke et al., 2019), and Tensorflow (Abadi et al., 2016). We used framework defaults for loading data, defining a layer, a loss function, an optimizer and a training loop. Whenever the framework provides all necessary features, it is generally a good idea to use them, since the library implementations of these components tend to be heavily optimized for performance and properly tested for reliability. At the same time, try not to forget that these modules can be implemented directly. This is especially important for aspiring researchers who wish to live on the leading edge of model development, where you will be inventing new components that cannot possibly exist in any current library.\n\nIn PyTorch, the data module provides tools for data processing, the nn module defines a large number of neural network layers and common loss functions. We can initialize the parameters by replacing their values with methods ending with _. Note that we need to specify the input dimensions of the network. While this is trivial for now, it can have significant knock-on effects when we want to design complex networks with many layers. Careful considerations of how to parametrize these networks is needed to allow portability.\n\n### 3.5.6 Exercises\n\n1. How would you need to change the learning rate if you replace the aggregate loss over the minibatch with an average over the loss on the minibatch?\n\n2. Review the framework documentation to see which loss functions are provided. In particular, replace the squared loss with Huber’s robust loss function. That is, use the loss function\n\n$$l(y, y') = \\begin{cases} |y - y'| - \\frac{\\sigma}{2} & \\text{if } |y - y'| > \\sigma \\\\ \\frac{1}{2\\sigma}(y - y')^2 & \\text{otherwise} \\end{cases}$$\n\n(3.5.1)\n\n3. How do you access the gradient of the weights of the model?"
    },
    "152": {
      "text": "4. What is the effect on the solution if you change the learning rate and the number of epochs? Does it keep on improving?\n\n5. How does the solution change as you vary the amount of data generated?\n   1. Plot the estimation error for $\\hat{w} - w$ and $\\hat{b} - b$ as a function of the amount of data.\n      Hint: increase the amount of data logarithmically rather than linearly, i.e., 5, 10, 20, 50, ..., 10,000 rather than 1000, 2000, ..., 10,000.\n\n   2. Why is the suggestion in the hint appropriate?\n\n**Discussions**\\(^{80}\\).\n\n### 3.6 Generalization\n\nConsider two college students diligently preparing for their final exam. Commonly, this preparation will consist of practicing and testing their abilities by taking exams administered in previous years. Nonetheless, doing well on past exams is no guarantee that they will excel when it matters. For instance, imagine a student, Extraordinary Ellie, whose preparation consisted entirely of memorizing the answers to previous years’ exam questions. Even if Ellie were endowed with an extraordinary memory, and thus could perfectly recall the answer to any *previously seen* question, she might nevertheless freeze when faced with a new (*previously unseen*) question. By comparison, imagine another student, Inductive Irene, with comparably poor memorization skills, but a knack for picking up patterns. Note that if the exam truly consisted of recycled questions from a previous year, Ellie would handily outperform Irene. Even if Irene’s inferred patterns yielded 90% accurate predictions, they could never compete with Ellie’s 100% recall. However, even if the exam consisted entirely of fresh questions, Irene might maintain her 90% average.\n\nAs machine learning scientists, our goal is to discover *patterns*. But how can we be sure that we have truly discovered a *general* pattern and not simply memorized our data? Most of the time, our predictions are only useful if our model discovers such a pattern. We do not want to predict yesterday’s stock prices, but tomorrow’s. We do not need to recognize already diagnosed diseases for previously seen patients, but rather previously undiagnosed ailments in previously unseen patients. This problem—how to discover patterns that generalize—is the fundamental problem of machine learning, and arguably of all of statistics. We might cast this problem as just one slice of a far grander question that engulfs all of science: when are we ever justified in making the leap from particular observations to more general statements?\n\nIn real life, we must fit our models using a finite collection of data. The typical scales of that data vary wildly across domains. For many important medical problems, we can only access a few thousand data points. When studying rare diseases, we might be lucky to access hundreds. By contrast, the largest public datasets consisting of labeled photographs, e.g., ImageNet (Deng et al., 2009), contain millions of images. And some unlabeled image"
    },
    "153": {
      "text": "collections such as the Flickr YFC100M dataset can be even larger, containing over 100 million images (Thomee et al., 2016). However, even at this extreme scale, the number of available data points remains infinitesimally small compared to the space of all possible images at a megapixel resolution. Whenever we work with finite samples, we must keep in mind the risk that we might fit our training data, only to discover that we failed to discover a generalizable pattern.\n\nThe phenomenon of fitting closer to our training data than to the underlying distribution is called *overfitting*, and techniques for combatting overfitting are often called *regularization* methods. While it is no substitute for a proper introduction to statistical learning theory (see Boucheron et al. (2005), Vapnik (1998)), we will give you just enough intuition to get going. We will revisit generalization in many chapters throughout the book, exploring both what is known about the principles underlying generalization in various models, and also heuristic techniques that have been found (empirically) to yield improved generalization on tasks of practical interest.\n\n### 3.6.1 Training Error and Generalization Error\n\nIn the standard supervised learning setting, we assume that the training data and the test data are drawn *independently* from *identical* distributions. This is commonly called the *IID assumption*. While this assumption is strong, it is worth noting that, absent any such assumption, we would be dead in the water. Why should we believe that training data sampled from distribution $P(X,Y)$ should tell us how to make predictions on test data generated by a *different distribution* $Q(X,Y)$? Making such leaps turns out to require strong assumptions about how $P$ and $Q$ are related. Later on we will discuss some assumptions that allow for shifts in distribution but first we need to understand the IID case, where $P(\\cdot) = Q(\\cdot)$.\n\nTo begin with, we need to differentiate between the *training error* $R_{\\text{emp}}$, which is a *statistic* calculated on the training dataset, and the *generalization error* $R$, which is an *expectation* taken with respect to the underlying distribution. You can think of the generalization error as what you would see if you applied your model to an infinite stream of additional data examples drawn from the same underlying data distribution. Formally the training error is expressed as a *sum* (with the same notation as Section 3.1):\n\n$$R_{\\text{emp}}[\\mathbf{X}, \\mathbf{y}, f] = \\frac{1}{n} \\sum_{i=1}^{n} l(\\mathbf{x}^{(i)}, y^{(i)}, f(\\mathbf{x}^{(i)})),$$\n\nwhile the generalization error is expressed as an integral:\n\n$$R[p, f] = E_{(x,y) \\sim P}[l(x, y, f(x))] = \\int \\int l(x, y, f(x)) p(x, y) \\, dx dy.$$\n\nProblematically, we can never calculate the generalization error $R$ exactly. Nobody ever tells us the precise form of the density function $p(x, y)$. Moreover, we cannot sample an infinite stream of data points. Thus, in practice, we must *estimate* the generalization error by applying our model to an independent test set constituted of a random selection of examples $\\mathbf{X}'$ and labels $\\mathbf{y}'$ that were withheld from our training set. This consists of"
    },
    "154": {
      "text": "applying the same formula that was used for calculating the empirical training error but to a test set \\( \\mathbf{X}' \\), \\( \\mathbf{y}' \\).\n\nCrucially, when we evaluate our classifier on the test set, we are working with a fixed classifier (it does not depend on the sample of the test set), and thus estimating its error is simply the problem of mean estimation. However the same cannot be said for the training set. Note that the model we wind up with depends explicitly on the selection of the training set and thus the training error will in general be a biased estimate of the true error on the underlying population. The central question of generalization is then when should we expect our training error to be close to the population error (and thus the generalization error).\n\n**Model Complexity**\n\nIn classical theory, when we have simple models and abundant data, the training and generalization errors tend to be close. However, when we work with more complex models and/or fewer examples, we expect the training error to go down but the generalization gap to grow. This should not be surprising. Imagine a model class so expressive that for any dataset of \\( n \\) examples, we can find a set of parameters that can perfectly fit arbitrary labels, even if randomly assigned. In this case, even if we fit our training data perfectly, how can we conclude anything about the generalization error? For all we know, our generalization error might be no better than random guessing.\n\nIn general, absent any restriction on our model class, we cannot conclude, based on fitting the training data alone, that our model has discovered any generalizable pattern (Vapnik et al., 1994). On the other hand, if our model class was not capable of fitting arbitrary labels, then it must have discovered a pattern. Learning-theoretic ideas about model complexity derived some inspiration from the ideas of Karl Popper, an influential philosopher of science, who formalized the criterion of falsifiability. According to Popper, a theory that can explain any and all observations is not a scientific theory at all! After all, what has it told us about the world if it has not ruled out any possibility? In short, what we want is a hypothesis that *could not* explain any observations we might conceivably make and yet nevertheless happens to be compatible with those observations that we *in fact* make.\n\nNow what precisely constitutes an appropriate notion of model complexity is a complex matter. Often, models with more parameters are able to fit a greater number of arbitrarily assigned labels. However, this is not necessarily true. For instance, kernel methods operate in spaces with infinite numbers of parameters, yet their complexity is controlled by other means (Schölkopf and Smola, 2002). One notion of complexity that often proves useful is the range of values that the parameters can take. Here, a model whose parameters are permitted to take arbitrary values would be more complex. We will revisit this idea in the next section, when we introduce weight decay, your first practical regularization technique. Notably, it can be difficult to compare complexity among members of substantially different model classes (say, decision trees vs. neural networks).\n\nAt this point, we must stress another important point that we will revisit when introducing deep neural networks. When a model is capable of fitting arbitrary labels, low training error does not necessarily imply low generalization error. However, it does not necessarily"
    },
    "155": {
      "text": "imply high generalization error either! All we can say with confidence is that low training error alone is not enough to certify low generalization error. Deep neural networks turn out to be just such models: while they generalize well in practice, they are too powerful to allow us to conclude much on the basis of training error alone. In these cases we must rely more heavily on our holdout data to certify generalization after the fact. Error on the holdout data, i.e., validation set, is called the *validation error*.\n\n### 3.6.2 Underfitting or Overfitting?\n\nWhen we compare the training and validation errors, we want to be mindful of two common situations. First, we want to watch out for cases when our training error and validation error are both substantial but there is a little gap between them. If the model is unable to reduce the training error, that could mean that our model is too simple (i.e., insufficiently expressive) to capture the pattern that we are trying to model. Moreover, since the *generalization gap* ($R_{\\text{emp}} - R$) between our training and generalization errors is small, we have reason to believe that we could get away with a more complex model. This phenomenon is known as *underfitting*.\n\nOn the other hand, as we discussed above, we want to watch out for the cases when our training error is significantly lower than our validation error, indicating severe *overfitting*. Note that overfitting is not always a bad thing. In deep learning especially, the best predictive models often perform far better on training data than on holdout data. Ultimately, we usually care about driving the generalization error lower, and only care about the gap insofar as it becomes an obstacle to that end. Note that if the training error is zero, then the generalization gap is precisely equal to the generalization error and we can make progress only by reducing the gap.\n\n### Polynomial Curve Fitting\n\nTo illustrate some classical intuition about overfitting and model complexity, consider the following: given training data consisting of a single feature $x$ and a corresponding real-valued label $y$, we try to find the polynomial of degree $d$\n\n$$\\hat{y} = \\sum_{i=0}^{d} x^i w_i$$\n\n(3.6.3)\n\nfor estimating the label $y$. This is just a linear regression problem where our features are given by the powers of $x$, the model’s weights are given by $w_i$, and the bias is given by $w_0$ since $x^0 = 1$ for all $x$. Since this is just a linear regression problem, we can use the squared error as our loss function.\n\nA higher-order polynomial function is more complex than a lower-order polynomial function, since the higher-order polynomial has more parameters and the model function’s selection range is wider. Fixing the training dataset, higher-order polynomial functions should always achieve lower (at worst, equal) training error relative to lower-degree polynomials. In fact, whenever each data example has a distinct value of $x$, a polynomial function with degree equal to the number of data examples can fit the training set perfectly. We compare"
    },
    "156": {
      "text": "the relationship between polynomial degree (model complexity) and both underfitting and overfitting in Fig. 3.6.1.\n\n![Fig. 3.6.1 Influence of model complexity on underfitting and overfitting.](image)\n\n**Dataset Size**\n\nAs the above bound already indicates, another big consideration to bear in mind is dataset size. Fixing our model, the fewer samples we have in the training dataset, the more likely (and more severely) we are to encounter overfitting. As we increase the amount of training data, the generalization error typically decreases. Moreover, in general, more data never hurts. For a fixed task and data distribution, model complexity should not increase more rapidly than the amount of data. Given more data, we might attempt to fit a more complex model. Absent sufficient data, simpler models may be more difficult to beat. For many tasks, deep learning only outperforms linear models when many thousands of training examples are available. In part, the current success of deep learning owes considerably to the abundance of massive datasets arising from Internet companies, cheap storage, connected devices, and the broad digitization of the economy.\n\n**3.6.3 Model Selection**\n\nTypically, we select our final model only after evaluating multiple models that differ in various ways (different architectures, training objectives, selected features, data preprocessing, learning rates, etc.). Choosing among many models is aptly called *model selection*.\n\nIn principle, we should not touch our test set until after we have chosen all our hyperparameters. Were we to use the test data in the model selection process, there is a risk that we might overfit the test data. Then we would be in serious trouble. If we overfit our training data, there is always the evaluation on test data to keep us honest. But if we overfit the test data, how would we ever know? See Ong et al. (2005) for an example of how this can lead to absurd results even for models where the complexity can be tightly controlled.\n\nThus, we should never rely on the test data for model selection. And yet we cannot rely solely on the training data for model selection either because we cannot estimate the generalization error on the very data that we use to train the model.\n\nIn practical applications, the picture gets muddier. While ideally we would only touch the"
    },
    "157": {
      "text": "test data once, to assess the very best model or to compare a small number of models with each other, real-world test data is seldom discarded after just one use. We can seldom afford a new test set for each round of experiments. In fact, recycling benchmark data for decades can have a significant impact on the development of algorithms, e.g., for image classification\\(^{81}\\) and optical character recognition\\(^{82}\\).\n\nThe common practice for addressing the problem of training on the test set is to split our data three ways, incorporating a validation set in addition to the training and test datasets. The result is a murky business where the boundaries between validation and test data are worryingly ambiguous. Unless explicitly stated otherwise, in the experiments in this book we are really working with what should rightly be called training data and validation data, with no true test sets. Therefore, the accuracy reported in each experiment of the book is really the validation accuracy and not a true test set accuracy.\n\n**Cross-Validation**\n\nWhen training data is scarce, we might not even be able to afford to hold out enough data to constitute a proper validation set. One popular solution to this problem is to employ \\(K\\)-fold cross-validation. Here, the original training data is split into \\(K\\) non-overlapping subsets. Then model training and validation are executed \\(K\\) times, each time training on \\(K - 1\\) subsets and validating on a different subset (the one not used for training in that round). Finally, the training and validation errors are estimated by averaging over the results from the \\(K\\) experiments.\n\n**3.6.4 Summary**\n\nThis section explored some of the underpinnings of generalization in machine learning. Some of these ideas become complicated and counterintuitive when we get to deeper models; here, models are capable of overfitting data badly, and the relevant notions of complexity can be both implicit and counterintuitive (e.g., larger architectures with more parameters generalizing better). We leave you with a few rules of thumb:\n\n1. Use validation sets (or \\(K\\)-fold cross-validation) for model selection;\n2. More complex models often require more data;\n3. Relevant notions of complexity include both the number of parameters and the range of values that they are allowed to take;\n4. Keeping all else equal, more data almost always leads to better generalization;\n5. This entire talk of generalization is all predicated on the IID assumption. If we relax this assumption, allowing for distributions to shift between the train and testing periods, then we cannot say anything about generalization absent a further (perhaps milder) assumption.\n\n**3.6.5 Exercises**\n\n1. When can you solve the problem of polynomial regression exactly?"
    },
    "158": {
      "text": "2. Give at least five examples where dependent random variables make treating the problem as IID data inadvisable.\n\n3. Can you ever expect to see zero training error? Under which circumstances would you see zero generalization error?\n\n4. Why is $K$-fold cross-validation very expensive to compute?\n\n5. Why is the $K$-fold cross-validation error estimate biased?\n\n6. The VC dimension is defined as the maximum number of points that can be classified with arbitrary labels $\\{ \\pm 1 \\}$ by a function of a class of functions. Why might this not be a good idea for measuring how complex the class of functions is? Hint: consider the magnitude of the functions.\n\n7. Your manager gives you a difficult dataset on which your current algorithm does not perform so well. How would you justify to him that you need more data? Hint: you cannot increase the data but you can decrease it.\n\nDiscussions\\(^83\\).\n\n3.7 Weight Decay\n\nNow that we have characterized the problem of overfitting, we can introduce our first regularization technique. Recall that we can always mitigate overfitting by collecting more training data. However, that can be costly, time consuming, or entirely out of our control, making it impossible in the short run. For now, we can assume that we already have as much high-quality data as our resources permit and focus the tools at our disposal when the dataset is taken as a given.\n\nRecall that in our polynomial regression example (Section 3.6.2) we could limit our model’s capacity by tweaking the degree of the fitted polynomial. Indeed, limiting the number of features is a popular technique for mitigating overfitting. However, simply tossing aside features can be too blunt an instrument. Sticking with the polynomial regression example, consider what might happen with high-dimensional input. The natural extensions of polynomials to multivariate data are called monomials, which are simply products of powers of variables. The degree of a monomial is the sum of the powers. For example, $x_1^2 x_2$, and $x_3 x_5^2$ are both monomials of degree 3.\n\nNote that the number of terms with degree $d$ blows up rapidly as $d$ grows larger. Given $k$ variables, the number of monomials of degree $d$ is $\\binom{k-1+d}{k-1}$. Even small changes in degree, say from 2 to 3, dramatically increase the complexity of our model. Thus we often need a more fine-grained tool for adjusting function complexity."
    },
    "159": {
      "text": "%matplotlib inline\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\n## 3.7.1 Norms and Weight Decay\n\nRather than directly manipulating the number of parameters, *weight decay*, operates by restricting the values that the parameters can take. More commonly called $\\ell_2$ regularization outside of deep learning circles when optimized by minibatch stochastic gradient descent, weight decay might be the most widely used technique for regularizing parametric machine learning models. The technique is motivated by the basic intuition that among all functions $f$, the function $f = 0$ (assigning the value 0 to all inputs) is in some sense the *simplest*, and that we can measure the complexity of a function by the distance of its parameters from zero. But how precisely should we measure the distance between a function and zero? There is no single right answer. In fact, entire branches of mathematics, including parts of functional analysis and the theory of Banach spaces, are devoted to addressing such issues.\n\nOne simple interpretation might be to measure the complexity of a linear function $f(x) = w^T x$ by some norm of its weight vector, e.g., $||w||^2$. Recall that we introduced the $\\ell_2$ norm and $\\ell_1$ norm, which are special cases of the more general $\\ell_p$ norm, in Section 2.3.11. The most common method for ensuring a small weight vector is to add its norm as a penalty term to the problem of minimizing the loss. Thus we replace our original objective, *minimizing the prediction loss on the training labels*, with new objective, *minimizing the sum of the prediction loss and the penalty term*. Now, if our weight vector grows too large, our learning algorithm might focus on minimizing the weight norm $||w||^2$ rather than minimizing the training error. That is exactly what we want. To illustrate things in code, we revive our previous example from Section 3.1 for linear regression. There, our loss was given by\n\n$$L(w, b) = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{2} \\left( w^T x^{(i)} + b - y^{(i)} \\right)^2.$$  (3.7.1)\n\nRecall that $x^{(i)}$ are the features, $y^{(i)}$ is the label for any data example $i$, and $(w, b)$ are the weight and bias parameters, respectively. To penalize the size of the weight vector, we must somehow add $||w||^2$ to the loss function, but how should the model trade off the standard loss for this new additive penalty? In practice, we characterize this trade-off via the *regularization constant* $\\lambda$, a nonnegative hyperparameter that we fit using validation data:\n\n$$L(w, b) + \\frac{\\lambda}{2} ||w||^2.$$  (3.7.2)\n\nFor $\\lambda = 0$, we recover our original loss function. For $\\lambda > 0$, we restrict the size of $||w||$. We divide by 2 by convention: when we take the derivative of a quadratic function, the 2 and 1/2 cancel out, ensuring that the expression for the update looks nice and simple. The astute reader might wonder why we work with the squared norm and not the standard"
    },
    "160": {
      "text": "norm (i.e., the Euclidean distance). We do this for computational convenience. By squaring the \\( \\ell_2 \\) norm, we remove the square root, leaving the sum of squares of each component of the weight vector. This makes the derivative of the penalty easy to compute: the sum of derivatives equals the derivative of the sum.\n\nMoreover, you might ask why we work with the \\( \\ell_2 \\) norm in the first place and not, say, the \\( \\ell_1 \\) norm. In fact, other choices are valid and popular throughout statistics. While \\( \\ell_2 \\)-regularized linear models constitute the classic **ridge regression** algorithm, \\( \\ell_1 \\)-regularized linear regression is a similarly fundamental method in statistics, popularly known as **lasso regression**. One reason to work with the \\( \\ell_2 \\) norm is that it places an outsize penalty on large components of the weight vector. This biases our learning algorithm towards models that distribute weight evenly across a larger number of features. In practice, this might make them more robust to measurement error in a single variable. By contrast, \\( \\ell_1 \\) penalties lead to models that concentrate weights on a small set of features by clearing the other weights to zero. This gives us an effective method for **feature selection**, which may be desirable for other reasons. For example, if our model only relies on a few features, then we may not need to collect, store, or transmit data for the other (dropped) features.\n\nUsing the same notation in (3.1.11), minibatch stochastic gradient descent updates for \\( \\ell_2 \\)-regularized regression as follows:\n\n\\[\n\\mathbf{w} \\leftarrow (1 - \\eta \\lambda) \\mathbf{w} - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{x}^{(i)} \\left( \\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)} \\right).\n\\]\n\nAs before, we update \\( \\mathbf{w} \\) based on the amount by which our estimate differs from the observation. However, we also shrink the size of \\( \\mathbf{w} \\) towards zero. That is why the method is sometimes called “weight decay”: given the penalty term alone, our optimization algorithm **decays** the weight at each step of training. In contrast to feature selection, weight decay offers us a mechanism for continuously adjusting the complexity of a function. Smaller values of \\( \\lambda \\) correspond to less constrained \\( \\mathbf{w} \\), whereas larger values of \\( \\lambda \\) constrain \\( \\mathbf{w} \\) more considerably. Whether we include a corresponding bias penalty \\( b^2 \\) can vary across implementations, and may vary across layers of a neural network. Often, we do not regularize the bias term. Besides, although \\( \\ell_2 \\) regularization may not be equivalent to weight decay for other optimization algorithms, the idea of regularization through shrinking the size of weights still holds true.\n\n### 3.7.2 High-Dimensional Linear Regression\n\nWe can illustrate the benefits of weight decay through a simple synthetic example.\n\nFirst, we generate some data as before:\n\n\\[\ny = 0.05 + \\sum_{i=1}^{d} 0.01 x_i + \\epsilon \\text{ where } \\epsilon \\sim \\mathcal{N}(0, 0.01^2).\n\\]\n\nIn this synthetic dataset, our label is given by an underlying linear function of our inputs, corrupted by Gaussian noise with zero mean and standard deviation 0.01. For illustrative purposes, we can make the effects of overfitting pronounced, by increasing the dimen-"
    },
    "161": {
      "text": "sionality of our problem to $d = 200$ and working with a small training set with only 20 examples.\n\n```python\nclass Data(d2l.DataModule):\n    def __init__(self, num_train, num_val, num_inputs, batch_size):\n        self.save_hyperparameters()\n        n = num_train + num_val\n        self.X = torch.randn(n, num_inputs)\n        noise = torch.randn(n, 1) * 0.01\n        w, b = torch.ones((num_inputs, 1)) * 0.01, 0.05\n        self.y = torch.matmul(self.X, w) + b + noise\n\n    def get_data_loader(self, train):\n        i = slice(0, self.num_train) if train else slice(self.num_train, None)\n        return self.get_tensor_loader([self.X, self.y], train, i)\n```\n\n3.7.3 Implementation from Scratch\n\nNow, let’s try implementing weight decay from scratch. Since minibatch stochastic gradient descent is our optimizer, we just need to add the squared $\\ell_2$ penalty to the original loss function.\n\nDefining $\\ell_2$ Norm Penalty\n\nPerhaps the most convenient way of implementing this penalty is to square all terms in place and sum them.\n\n```python\ndef l2_penalty(w):\n    return (w ** 2).sum() / 2\n```\n\nDefining the Model\n\nIn the final model, the linear regression and the squared loss have not changed since Section 3.4, so we will just define a subclass of d2l.LinearRegressionScratch. The only change here is that our loss now includes the penalty term.\n\n```python\nclass WeightDecayScratch(d2l.LinearRegressionScratch):\n    def __init__(self, num_inputs, lambd, lr, sigma=0.01):\n        super().__init__(num_inputs, lr, sigma)\n        self.save_hyperparameters()\n\n    def loss(self, y_hat, y):\n        return (super().loss(y_hat, y) +\n                self.lambd * l2_penalty(self.w))\n```\n\nThe following code fits our model on the training set with 20 examples and evaluates it on the validation set with 100 examples."
    },
    "162": {
      "text": "data = Data(num_train=20, num_val=100, num_inputs=200, batch_size=5)\ntrainer = d2l.Trainer(max_epochs=10)\n\ndef train_scratch(lambd):\n    model = WeightDecayScratch(num_inputs=200, lambd=lambd, lr=0.01)\n    model.board.yscale='log'\n    trainer.fit(model, data)\n    print('L2 norm of w:', float(l2_penalty(model.w)))\n\nTraining without Regularization\n\nWe now run this code with lambd = 0, disabling weight decay. Note that we overfit badly, decreasing the training error but not the validation error—a textbook case of overfitting.\n\ntrain_scratch(0)\n\nL2 norm of w: 0.009948714636266232\n\nUsing Weight Decay\n\nBelow, we run with substantial weight decay. Note that the training error increases but the validation error decreases. This is precisely the effect we expect from regularization.\n\ntrain_scratch(3)\n\nL2 norm of w: 0.0017270983662456274\n\n3.7.4 Concise Implementation\n\nBecause weight decay is ubiquitous in neural network optimization, the deep learning framework makes it especially convenient, integrating weight decay into the optimization"
    },
    "163": {
      "text": "algorithm itself for easy use in combination with any loss function. Moreover, this integration serves a computational benefit, allowing implementation tricks to add weight decay to the algorithm, without any additional computational overhead. Since the weight decay portion of the update depends only on the current value of each parameter, the optimizer must touch each parameter once anyway.\n\nBelow, we specify the weight decay hyperparameter directly through weight_decay when instantiating our optimizer. By default, PyTorch decays both weights and biases simultaneously, but we can configure the optimizer to handle different parameters according to different policies. Here, we only set weight_decay for the weights (the net.weight parameters), hence the bias (the net.bias parameter) will not decay.\n\n```python\nclass WeightDecay(d2l.LinearRegression):\n    def __init__(self, wd, lr):\n        super().__init__(lr)\n        self.save_hyperparameters()\n        self.wd = wd\n\n    def configure_optimizers(self):\n        return torch.optim.SGD([\n            {'params': self.net.weight, 'weight_decay': self.wd},\n            {'params': self.net.bias}], lr=self.lr)\n```\n\nThe plot looks similar to that when we implemented weight decay from scratch. However, this version runs faster and is easier to implement, benefits that will become more pronounced as you address larger problems and this work becomes more routine.\n\n```python\nmodel = WeightDecay(wd=3, lr=0.01)\nmodel.board.yscale='log'\ntrainer.fit(model, data)\n\nprint('L2 norm of w:', float(l2_penalty(model.get_w_b()[0])))\n```\n\nL2 norm of w: 0.013779522851109505\n\nSo far, we have touched upon one notion of what constitutes a simple linear function. However, even for simple nonlinear functions, the situation can be much more complex. To see this, the concept of reproducing kernel Hilbert space (RKHS) allows one to apply tools"
    },
    "164": {
      "text": "introduced for linear functions in a nonlinear context. Unfortunately, RKHS-based algorithms tend to scale poorly to large, high-dimensional data. In this book we will often adopt the common heuristic whereby weight decay is applied to all layers of a deep network.\n\n3.7.5 Summary\n\nRegularization is a common method for dealing with overfitting. Classical regularization techniques add a penalty term to the loss function (when training) to reduce the complexity of the learned model. One particular choice for keeping the model simple is using an $\\ell_2$ penalty. This leads to weight decay in the update steps of the minibatch stochastic gradient descent algorithm. In practice, the weight decay functionality is provided in optimizers from deep learning frameworks. Different sets of parameters can have different update behaviors within the same training loop.\n\n3.7.6 Exercises\n\n1. Experiment with the value of $\\lambda$ in the estimation problem in this section. Plot training and validation accuracy as a function of $\\lambda$. What do you observe?\n\n2. Use a validation set to find the optimal value of $\\lambda$. Is it really the optimal value? Does this matter?\n\n3. What would the update equations look like if instead of $||\\mathbf{w}||^2$ we used $\\sum_i |w_i|$ as our penalty of choice ($\\ell_1$ regularization)?\n\n4. We know that $||\\mathbf{w}||^2 = \\mathbf{w}^\\top \\mathbf{w}$. Can you find a similar equation for matrices (see the Frobenius norm in Section 2.3.11)?\n\n5. Review the relationship between training error and generalization error. In addition to weight decay, increased training, and the use of a model of suitable complexity, what other ways might help us deal with overfitting?\n\n6. In Bayesian statistics we use the product of prior and likelihood to arrive at a posterior via $P(w|x) \\propto P(x|w)P(w)$. How can you identify $P(w)$ with regularization?\n\nDiscussions\\(^{85}\\)."
    }
  }
}